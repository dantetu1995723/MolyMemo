import Foundation
@preconcurrency import AVFoundation

/// æŒ‰ä½è¯´è¯ï¼šç›´æ¥é‡‡é›†éº¦å…‹é£ PCMï¼ˆç»Ÿä¸€è¾“å‡º 16k/16bit/mono çš„ Int16 PCM bytesï¼‰
final class HoldToTalkPCMRecorder: ObservableObject {
    enum RecorderError: LocalizedError {
        case micPermissionDenied
        case cannotCreateConverter
        case engineStartFailed

        var errorDescription: String? {
            switch self {
            case .micPermissionDenied: return "éº¦å…‹é£æƒé™æœªæˆæƒ"
            case .cannotCreateConverter: return "æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨"
            case .engineStartFailed: return "éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥"
            }
        }
    }

    @Published private(set) var isRecording: Bool = false
    @Published private(set) var audioLevel: Float = 0

    private let audioQueue = DispatchQueue(label: "com.molymemo.holdtotalk.pcm")
    private let engine = AVAudioEngine()
    private var converter: AVAudioConverter?

    // ä»…åœ¨ audioQueue å†…è¯»å†™ï¼Œé¿å…ä¸éŸ³é¢‘ tap çº¿ç¨‹æŠ¢æ•°æ®
    private var pcmData = Data()
    private var bytesPerFrame: Int = 2 // int16 mono

    // MARK: - Main-thread publishing helpers
    // SwiftUI è¦æ±‚ @Published çš„å˜æ›´å¿…é¡»åœ¨ä¸»çº¿ç¨‹å‘å¸ƒï¼Œå¦åˆ™ä¼šå‡ºç°ç´«è‰²è¿è¡Œæ—¶æŠ¥è­¦ã€‚
    private func publishIsRecording(_ value: Bool) {
        if Thread.isMainThread {
            isRecording = value
        } else {
            DispatchQueue.main.sync {
                self.isRecording = value
            }
        }
    }

    private func publishAudioLevel(_ value: Float) {
        if Thread.isMainThread {
            audioLevel = value
        } else {
            DispatchQueue.main.sync {
                self.audioLevel = value
            }
        }
    }

    func start() async throws {
        if isRecording {
            _ = stop(discard: false)
        }

        let granted = await requestMicPermission()
        guard granted else { throw RecorderError.micPermissionDenied }

        try configureAudioSessionForRecording()

        pcmData = Data()
        // @Published æ›´æ–°å›åˆ°ä¸»çº¿ç¨‹ï¼Œé¿å… SwiftUI æŠ¥è­¦
        publishAudioLevel(0)

        let inputNode = engine.inputNode
        let bus = 0
        inputNode.removeTap(onBus: bus)

        let inFormat = inputNode.inputFormat(forBus: bus)
        guard let outFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 16_000, channels: 1, interleaved: false) else {
            throw RecorderError.cannotCreateConverter
        }
        guard let conv = AVAudioConverter(from: inFormat, to: outFormat) else {
            throw RecorderError.cannotCreateConverter
        }
        converter = conv
        bytesPerFrame = MemoryLayout<Int16>.size // mono

        let outFrameCapacity: AVAudioFrameCount = 2048
        guard let outBuffer = AVAudioPCMBuffer(pcmFormat: outFormat, frameCapacity: outFrameCapacity) else {
            throw RecorderError.cannotCreateConverter
        }

        publishIsRecording(true)
        print("[HoldToTalk] ğŸ™ï¸ start PCM engine capture (in=\(inFormat.sampleRate)Hz ch=\(inFormat.channelCount))")

        // æ•è·å¿…è¦å¯¹è±¡ï¼Œé¿å…åœ¨ @Sendable é—­åŒ…é‡Œç›´æ¥è§¦ç¢° main-actor çŠ¶æ€
        let q = audioQueue
        inputNode.installTap(onBus: bus, bufferSize: 1024, format: inFormat) { [weak self] buffer, _ in
            // 1) è®¡ç®—éŸ³é‡ï¼ˆç”¨è¾“å…¥ buffer æ›´å®æ—¶ï¼‰ï¼Œå›åˆ°ä¸»çº¿ç¨‹æ›´æ–° UI
            let level = Self.computeLevel(buffer: buffer)
            DispatchQueue.main.async { [weak self] in
                self?.audioLevel = level
            }

            // 2) è½¬æˆ 16k/int16/monoï¼Œå¹¶æŠŠ bytes è¿½åŠ åˆ°å†…å­˜ï¼ˆè¿½åŠ æ“ä½œæ”¾åˆ°ä¸²è¡Œé˜Ÿåˆ—ï¼Œé¿å…æ•°æ®ç«äº‰ï¼‰
            q.async { [weak self] in
                guard let self else { return }
                guard self.isRecording else { return }
                guard let converter = self.converter else { return }

                outBuffer.frameLength = 0
                var error: NSError?
                // å…³é”®ï¼šAVAudioConverter åœ¨ä¸€æ¬¡ convert() è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå¤šæ¬¡è°ƒç”¨ inputBlock æ‹‰å–è¾“å…¥ã€‚
                // å¦‚æœæˆ‘ä»¬æ¯æ¬¡éƒ½è¿”å›åŒä¸€ä¸ª bufferï¼Œä¼šå¯¼è‡´åŒä¸€æ®µéŸ³é¢‘è¢«â€œé‡å¤å–‚å…¥â€ï¼Œå¬èµ·æ¥åƒå›å£°/é‡å½±ã€‚
                // æ­£ç¡®åšæ³•ï¼šå¯¹â€œå•ä¸ªè¾“å…¥ buffer çš„è½¬æ¢â€ï¼Œåªæä¾›ä¸€æ¬¡è¾“å…¥ï¼Œåç»­è¿”å› endOfStreamã€‚
                var didProvideInput = false
                converter.reset() // é¿å…è·¨ buffer çš„å†…éƒ¨çŠ¶æ€æ®‹ç•™ï¼ˆæ›´ç¡®å®šçš„å•æ®µè½¬æ¢ï¼‰
                let status = converter.convert(to: outBuffer, error: &error) { _, outStatus -> AVAudioBuffer? in
                    if didProvideInput {
                        outStatus.pointee = .endOfStream
                        return nil
                    }
                    didProvideInput = true
                    outStatus.pointee = .haveData
                    return buffer
                }
                if let error {
                    print("[HoldToTalk] âŒ PCM convert error -> \(error.domain)(\(error.code)) \(error.localizedDescription)")
                    return
                }
                guard status == .haveData, outBuffer.frameLength > 0 else { return }
                guard let p = outBuffer.int16ChannelData?[0] else { return }
                let byteCount = Int(outBuffer.frameLength) * self.bytesPerFrame
                let bytes = Data(bytes: p, count: byteCount)
                self.pcmData.append(bytes)
            }
        }

        engine.prepare()
        do {
            try engine.start()
        } catch {
            publishIsRecording(false)
            throw RecorderError.engineStartFailed
        }
    }

    /// - Returns: 16k/16bit/mono PCM bytesï¼ˆInt16 little-endianï¼‰
    func stop(discard: Bool) -> Data {
        let wasRecording = isRecording
        // å…ˆæŠŠå¯¹å¤–çŠ¶æ€åˆ‡å›â€œéå½•éŸ³â€ï¼Œä¸”å¿…é¡»åœ¨ä¸»çº¿ç¨‹å‘å¸ƒï¼ˆå¦åˆ™ SwiftUI æŠ¥è­¦ï¼‰
        publishIsRecording(false)
        publishAudioLevel(0)

        if engine.isRunning {
            engine.stop()
        }
        engine.inputNode.removeTap(onBus: 0)
        converter = nil

        // æ”¶å› AudioSession
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: [.notifyOthersOnDeactivation])
        } catch {
            // ignore
        }

        // ç­‰å¾…éŸ³é¢‘é˜Ÿåˆ—æŠŠå°¾å·´æ”¶å¹²å‡€ï¼Œé¿å…â€œæœ€åä¸€æ®µâ€ä¸¢å¤±
        let bytes: Data = audioQueue.sync {
            let out = pcmData
            pcmData = Data()
            return out
        }

        if wasRecording {
            if discard {
                print("[HoldToTalk] ğŸ›‘ stop PCM capture (discard)")
            } else {
                print("[HoldToTalk] ğŸ›‘ stop PCM capture bytes=\(bytes.count)")
            }
        }

        return discard ? Data() : bytes
    }

    /// å½•éŸ³è¿‡ç¨‹ä¸­å–å‡ºå½“å‰å·²ç¼“å­˜çš„ PCM bytesï¼Œå¹¶æ¸…ç©ºå†…éƒ¨ç¼“å­˜ï¼ˆç”¨äºæµå¼å‘é€ï¼‰ã€‚
    /// - Returns: 16k/16bit/mono PCM bytesï¼ˆInt16 little-endianï¼‰
    func drainPCMBytes() -> Data {
        audioQueue.sync {
            let out = pcmData
            pcmData = Data()
            return out
        }
    }

    // MARK: - Helpers

    private func requestMicPermission() async -> Bool {
        await withCheckedContinuation { continuation in
            if #available(iOS 17.0, *) {
                AVAudioApplication.requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            } else {
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    continuation.resume(returning: granted)
                }
            }
        }
    }

    private func configureAudioSessionForRecording() throws {
        let audioSession = AVAudioSession.sharedInstance()
        // æç®€â€œé€šè¯å¼â€é…ç½®ï¼ˆæ¥è¿‘ç³»ç»Ÿç”µè¯çš„ä½“éªŒï¼‰ï¼š
        // - å¿…é¡»ä½¿ç”¨ playAndRecord + voiceChat æ‰æœ‰ç³»ç»Ÿçš„è¯­éŸ³å¤„ç†ï¼ˆAEC/NS/AGCï¼‰
        // - ä¸å¼€å¯ defaultToSpeakerï¼šé»˜è®¤èµ°å¬ç­’ï¼ˆreceiverï¼‰ï¼Œé¿å…å¤–æ”¾å›çŒé€ æˆå›å£°
        // - ä¸ºäº†æœ€å¤§åŒ–æ¶ˆé™¤å›å£°ï¼Œè¿™é‡Œä¸å¯ç”¨è“ç‰™é€šè¯ï¼ˆallowBluetoothHFPï¼‰ï¼Œå¼ºåˆ¶ç”¨å†…ç½®éº¦å…‹é£+å¬ç­’
        try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.duckOthers])

        // å¼ºåˆ¶èµ°å¬ç­’ï¼ˆä¸æ˜¯æ‰¬å£°å™¨ï¼‰
        try audioSession.overrideOutputAudioPort(.none)

        // å°½é‡å›ºå®šç”¨å†…ç½®éº¦å…‹é£ï¼ˆé¿å…è“ç‰™/å¤šè·¯ç”±å¯¼è‡´çš„â€œä¾§éŸ³/å›å£°â€ä½“æ„Ÿï¼‰
        if #available(iOS 13.0, *) {
            if let builtInMic = audioSession.availableInputs?.first(where: { $0.portType == .builtInMic }) {
                try? audioSession.setPreferredInput(builtInMic)
            }
        }

        try? audioSession.setPreferredSampleRate(48_000)
        try? audioSession.setPreferredInputNumberOfChannels(1)
        try? audioSession.setPreferredIOBufferDuration(0.01)
        try audioSession.setActive(true)
    }

    private static func computeLevel(buffer: AVAudioPCMBuffer) -> Float {
        // ä¼˜å…ˆ floatChannelDataï¼›æ²¡æœ‰å°±é€€åŒ–
        if let ch = buffer.floatChannelData?[0] {
            let frames = Int(buffer.frameLength)
            guard frames > 0 else { return 0 }
            var sum: Float = 0
            var peak: Float = 0
            for i in 0..<frames {
                let s = abs(ch[i])
                sum += s * s
                peak = max(peak, s)
            }
            let rms = sqrt(sum / Float(frames))
            let raw = rms * 0.6 + peak * 0.4
            // æ”¾å®½å°å£°é—¨é™ï¼šè®©æ›´å°å£°ä¹Ÿèƒ½é©±åŠ¨ UIï¼ˆä¸å½±å“å®é™… PCM æ•°æ®ï¼‰
            let noiseFloor: Float = 0.008
            let normalized = max(0, raw - noiseFloor) / max(0.0001, 1 - noiseFloor)
            // å¢åŠ å¢ç›Šï¼šå°å£°æ›´å®¹æ˜“â€œèµ·æ³¢å½¢â€ï¼Œå¤§å£°ä»ä¼šè¢« clamp åˆ° 1.0
            let gained = min(normalized * 6.8, 1.0)
            return pow(gained, 0.55)
        }
        return 0
    }
}


