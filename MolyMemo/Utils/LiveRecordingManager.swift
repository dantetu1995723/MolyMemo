import Foundation
import AVFoundation
import Speech
import ActivityKit
import SwiftData
import UIKit

// å®æ—¶å½•éŸ³ç®¡ç†å™¨ - åŒæ—¶å½•éŸ³å’Œå®æ—¶è½¬å†™
class LiveRecordingManager: ObservableObject {
    static let shared = LiveRecordingManager()
    
    @Published var isRecording = false
    @Published var isPaused = false
    @Published var recognizedText = ""
    @Published var recordingDuration: TimeInterval = 0
    
    private var audioRecorder: AVAudioRecorder?
    private var recordingTimer: Timer?
    private var audioURL: URL?
    
    // Speech è¯†åˆ«å™¨
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // Live Activity
    private var activity: Activity<MeetingRecordingAttributes>?

    // Widget/å¿«æ·æŒ‡ä»¤åœºæ™¯ï¼šå¯ä»¥åªåœ¨åå°åšè½¬å†™ï¼Œä½†ä¸æŠŠæ–‡æœ¬æ¨åˆ° UIï¼ˆçµåŠ¨å²›/Live Activityï¼‰
    private var publishTranscriptionToUI: Bool = true
    
    // ä¿å­˜ ModelContext çš„å›è°ƒ
    var modelContextProvider: (() -> ModelContext?)?
    
    private init() {
        // ç›‘å¬appçŠ¶æ€å˜åŒ–ï¼Œç¡®ä¿åå°å½•éŸ³æ­£å¸¸
        setupBackgroundHandling()
        // å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
        cleanupStaleActivities()
    }
    
    // å¼€å§‹å½•éŸ³
    /// - Parameter publishTranscriptionToUI: æ˜¯å¦åœ¨ Live Activity / çµåŠ¨å²›æ˜¾ç¤ºå®æ—¶è½¬å†™æ–‡æœ¬ï¼ˆé»˜è®¤ trueï¼‰ã€‚
    func startRecording(publishTranscriptionToUI: Bool = true) {
        self.publishTranscriptionToUI = publishTranscriptionToUI
        print("[RecordingFlow] ğŸ™ï¸ startRecording publishToUI=\(publishTranscriptionToUI)")
        
        // è¯·æ±‚æƒé™
        requestPermissions { [weak self] granted in
            guard granted else {
                print("[RecordingFlow] âŒ startRecording permission denied")
                return
            }
            
            self?.setupRecording()
        }
    }
    
    private func requestPermissions(completion: @escaping (Bool) -> Void) {
        // è¯·æ±‚éº¦å…‹é£æƒé™ï¼ˆiOS 17 åŠä»¥ä¸Šä½¿ç”¨ AVAudioApplicationï¼‰
        let requestMicPermission: (@escaping (Bool) -> Void) -> Void = { handler in
            if #available(iOS 17.0, *) {
                AVAudioApplication.requestRecordPermission { granted in
                    handler(granted)
                }
            } else {
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    handler(granted)
                }
            }
        }

        requestMicPermission { micGranted in
            guard micGranted else {
                completion(false)
                return
            }

            // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    completion(authStatus == .authorized)
                }
            }
        }
    }
    
    private func setupRecording() {
        // é…ç½®éŸ³é¢‘ä¼šè¯ - æ”¯æŒåå°å½•éŸ³
        let audioSession = AVAudioSession.sharedInstance()
        do {
            let options: AVAudioSession.CategoryOptions = [
                .defaultToSpeaker,
                .allowBluetoothA2DP,
                .mixWithOthers
            ]
            try audioSession.setCategory(.playAndRecord, mode: .default, options: options)
            try audioSession.setActive(true)
        } catch {
            print("[RecordingFlow] âŒ setupRecording audioSession failed -> \(error.localizedDescription)")
            return
        }
        
        // å‡†å¤‡å½•éŸ³æ–‡ä»¶ï¼ˆç»Ÿä¸€å­˜æ”¾åœ¨ MeetingRecordings æ–‡ä»¶å¤¹ï¼‰
        let recordingsFolder = ensureRecordingsFolder()
        audioURL = recordingsFolder.appendingPathComponent("meeting_\(Int(Date().timeIntervalSince1970)).m4a")
        
        guard let audioURL = audioURL else { return }
        print("[RecordingFlow] ğŸ“ recording file = \(audioURL.path)")
        
        // é…ç½®å½•éŸ³è®¾ç½®ï¼ˆm4a AAC æ ¼å¼ï¼Œé«˜è´¨é‡å‹ç¼©ï¼‰
        let settings: [String: Any] = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 44100.0,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue,
            AVEncoderBitRateKey: 128000
        ]
        
        do {
            // åˆ›å»ºå½•éŸ³å™¨
            audioRecorder = try AVAudioRecorder(url: audioURL, settings: settings)
            audioRecorder?.record()
            print("[RecordingFlow] âœ… AVAudioRecorder started (m4a/AAC 44.1k 1ch)")
            
            // é‡ç½®çŠ¶æ€
            isRecording = true
            recognizedText = ""
            recordingDuration = 0
            
            // å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
            recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
                guard let self = self else { return }
                self.recordingDuration += 0.5
                self.updateLiveActivity()
            }
            RunLoop.current.add(recordingTimer!, forMode: .common)
            
            // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
            startSpeechRecognition()
            
            // å¯åŠ¨ Live Activity
            startLiveActivity()
            
        } catch {
            print("[RecordingFlow] âŒ AVAudioRecorder create/start failed -> \(error.localizedDescription)")
        }
    }
    
    // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
    private func startSpeechRecognition() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        // é…ç½®éŸ³é¢‘å¼•æ“
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    self.updateLiveActivity()
                }
            }
            
            if let error = error {
                let nsError = error as NSError
                if nsError.code != 301 {  // å¿½ç•¥å–æ¶ˆé”™è¯¯
                }
            }
        }
    }
    
    // æš‚åœå½•éŸ³
    func pauseRecording() {
        guard isRecording && !isPaused else { return }
        
        isPaused = true
        print("[RecordingFlow] â¸ï¸ pauseRecording")
        
        // æš‚åœå½•éŸ³å™¨
        audioRecorder?.pause()
        recordingTimer?.invalidate()
        
        // æš‚åœéŸ³é¢‘å¼•æ“
        audioEngine.pause()
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // ç»§ç»­å½•éŸ³
    func resumeRecording() {
        guard isRecording && isPaused else { return }
        
        isPaused = false
        print("[RecordingFlow] â–¶ï¸ resumeRecording")
        
        // ç»§ç»­å½•éŸ³å™¨
        audioRecorder?.record()
        
        // é‡æ–°å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
        recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            self.recordingDuration += 0.5
            self.updateLiveActivity()
        }
        RunLoop.current.add(recordingTimer!, forMode: .common)
        
        // ç»§ç»­éŸ³é¢‘å¼•æ“
        do {
            try audioEngine.start()
        } catch {
        }
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // åœæ­¢å½•éŸ³
    func stopRecording(modelContext: ModelContext? = nil) {
        
        guard isRecording else { 
            return 
        }
        print("[RecordingFlow] ğŸ›‘ stopRecording duration=\(recordingDuration)s recognizedTextLen=\(recognizedText.count)")
        
        isRecording = false
        isPaused = false
        
        // åœæ­¢å½•éŸ³å™¨
        audioRecorder?.stop()
        recordingTimer?.invalidate()
        
        // åœæ­¢éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        // å…³é”®ä¿®å¤ï¼šåœæ­¢å½•éŸ³åæ”¶å› AudioSessionï¼Œé¿å…åç»­æ’­æ”¾éŸ³è´¨å¼‚å¸¸/é…ç½®å¤±è´¥ï¼ˆOSStatus -50ï¼‰
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: [.notifyOthersOnDeactivation])
            #if DEBUG
            #endif
        } catch {
            #if DEBUG
            #endif
            print("[RecordingFlow] âš ï¸ audioSession deactivate failed -> \(error.localizedDescription)")
        }
        
        
        // è°ƒç”¨åç«¯APIç”Ÿæˆä¼šè®®çºªè¦
        uploadToBackend()
        
        // ç»“æŸ Live Activityï¼ˆå†…éƒ¨å·²åŒ…å«â€œå·²å®Œæˆâ€çŠ¶æ€å±•ç¤ºå’Œå»¶è¿Ÿé€»è¾‘ï¼‰
        endLiveActivity()
        
        
    }
    
    /// é€šçŸ¥ä¸»Appä¸Šä¼ éŸ³é¢‘åˆ°åç«¯ç”Ÿæˆä¼šè®®çºªè¦
    /// æ³¨æ„ï¼šè¿™é‡Œåªå‘é€é€šçŸ¥ï¼Œå®é™…çš„åç«¯è°ƒç”¨ç”±ä¸»Appå¤„ç†ï¼ˆå› ä¸ºWidget Extensionæ— æ³•è®¿é—®MeetingMinutesServiceï¼‰
    private func uploadToBackend() {
        guard let audioURL = audioURL else {
            return
        }
        print("[RecordingFlow] â˜ï¸ notify backend upload audioPath=\(audioURL.path)")
        
        let title = "Molyå½•éŸ³ - \(formatDate(Date()))"
        let date = Date()
        let duration = recordingDuration
        let audioPath = audioURL.path
        
        
        // å‘é€é€šçŸ¥ï¼Œè®©ä¸»Appå¤„ç†åç«¯ä¸Šä¼ 
        // RecordingNeedsUpload: ä¸»Appä¼šç›‘å¬è¿™ä¸ªé€šçŸ¥å¹¶è°ƒç”¨MeetingMinutesService
        let meetingData: [String: Any] = [
            "title": title,
            "date": date,
            "duration": duration,
            "audioPath": audioPath,
            "needsBackendUpload": true  // æ ‡è®°éœ€è¦åç«¯ä¸Šä¼ 
        ]
        
        DispatchQueue.main.async {
            NotificationCenter.default.post(
                name: NSNotification.Name("RecordingNeedsUpload"),
                object: nil,
                userInfo: meetingData
            )
        }
        
    }
    
    // MARK: - Live Activity ç®¡ç†
    
    private func startLiveActivity() {
        guard ActivityAuthorizationInfo().areActivitiesEnabled else {
            return
        }
        
        let attributes = MeetingRecordingAttributes(meetingTitle: "Molyå½•éŸ³")
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: publishTranscriptionToUI ? "å¼€å§‹å½•éŸ³..." : "",
            duration: 0,
            isRecording: true,
            isPaused: false
        )
        
        do {
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // æœ€é«˜ä¼˜å…ˆçº§ï¼Œä¿æŒå±•å¼€çŠ¶æ€
            )
            
            activity = try Activity<MeetingRecordingAttributes>.request(
                attributes: attributes,
                content: activityContent,
                pushType: nil
            )
        } catch {
        }
    }
    
    private func updateLiveActivity() {
        guard let activity = activity else { return }
        
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: {
                guard publishTranscriptionToUI else { return "" }
                return recognizedText.isEmpty ? "ç­‰å¾…è¯´è¯..." : recognizedText
            }(),
            duration: recordingDuration,
            isRecording: isRecording,
            isPaused: isPaused
        )
        
        Task { @MainActor in
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // ä¿æŒæœ€é«˜ä¼˜å…ˆçº§
            )
            await activity.update(activityContent)
        }
    }
    
    private func endLiveActivity() {
        guard let activity = activity else { return }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText,
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        // æ•è·å½“å‰çš„ activity å¼•ç”¨
        let currentActivity = activity
        
        Task {
            // 1. ç«‹å³æ›´æ–°åˆ°â€œå·²å®Œæˆâ€çŠ¶æ€ï¼ŒçµåŠ¨å²›ä¼šæ ¹æ® Widget é€»è¾‘æ˜¾ç¤ºç»¿è‰²å‹¾é€‰å’Œå®Œæˆæ–‡æ¡ˆ
            let updateContent = ActivityContent(
                state: finalState,
                staleDate: nil,
                relevanceScore: 100.0
            )
            await currentActivity.update(updateContent)
            
            // 2. åœç•™ 2.5 ç§’ï¼Œè®©ç”¨æˆ·æœ‰å……è¶³çš„æ—¶é—´æ„Ÿå—åˆ°å½•éŸ³å·²ç»æˆåŠŸç»“æŸå¹¶ä¿å­˜
            try? await Task.sleep(nanoseconds: 2_500_000_000)
            
            // 3. æ­£å¼å‘ŠçŸ¥ç³»ç»Ÿç»“æŸ Activity
            // dismissalPolicy è®¾ç½®ä¸º immediate å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨ä¸Šé¢ä¸»åŠ¨åœç•™è¿‡äº†
            // å¦‚æœæ˜¯åœ¨é”å±ç•Œé¢ï¼Œç³»ç»Ÿä¼šæ ¹æ®å…¶ç­–ç•¥å†³å®šæ˜¯å¦ç»§ç»­ä¿ç•™å°éƒ¨ä»¶
            if #available(iOS 16.2, *) {
                await currentActivity.end(updateContent, dismissalPolicy: .after(.now + 1.0))
            } else {
                await currentActivity.end(dismissalPolicy: .after(.now + 1.0))
            }
        }
        
        // ç½®ç©ºå®ä¾‹ï¼Œé˜²æ­¢é‡å¤æ“ä½œ
        self.activity = nil
    }
    
    // ç«‹å³å¼ºåˆ¶ç»“æŸLive Activityï¼ˆç”¨äºAppç»ˆæ­¢æ—¶ï¼‰
    private func endLiveActivityImmediately() {
        guard let activity = activity else { 
            // æ²¡æœ‰activityå®ä¾‹ï¼Œå°è¯•æ¸…ç†æ‰€æœ‰æ´»åŠ¨çš„Activity
            cleanupStaleActivities()
            return
        }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText,
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        let semaphore = DispatchSemaphore(value: 0)
        Task {
            if #available(iOS 16.2, *) {
                let content = ActivityContent(
                    state: finalState,
                    staleDate: nil,
                    relevanceScore: 100.0
                )
                await activity.end(content, dismissalPolicy: .immediate)
            } else {
                await activity.end(dismissalPolicy: .immediate)
            }
            semaphore.signal()
        }
        _ = semaphore.wait(timeout: .now() + 0.5)
        self.activity = nil
    }
    
    // æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
    private func cleanupStaleActivities() {
        
        Task { @MainActor in
            let activities = Activity<MeetingRecordingAttributes>.activities
            guard !activities.isEmpty else {
                return
            }
            
            for activity in activities {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: "",
                    duration: 0,
                    isRecording: false,
                    isPaused: false
                )
                if #available(iOS 16.2, *) {
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                } else {
                    await activity.end(dismissalPolicy: .immediate)
                }
            }
        }
    }
    
    // MARK: - åå°å¤„ç†
    
    private func setupBackgroundHandling() {
        // ç›‘å¬appè¿›å…¥åå°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppDidEnterBackground),
            name: UIApplication.didEnterBackgroundNotification,
            object: nil
        )
        
        // ç›‘å¬appè¿›å…¥å‰å°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillEnterForeground),
            name: UIApplication.willEnterForegroundNotification,
            object: nil
        )
        
        // ç›‘å¬appå³å°†ç»ˆæ­¢
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillTerminate),
            name: UIApplication.willTerminateNotification,
            object: nil
        )
        
        // ç›‘å¬éŸ³é¢‘ä¸­æ–­
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioInterruption),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„æš‚åœå‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handlePauseFromWidget),
            name: NSNotification.Name("PauseRecordingFromWidget"),
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„ç»§ç»­å‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleResumeFromWidget),
            name: NSNotification.Name("ResumeRecordingFromWidget"),
            object: nil
        )
    }
    
    @objc private func handleAppDidEnterBackground() {
        
        guard isRecording else { return }
        
        // ç¡®ä¿éŸ³é¢‘ä¼šè¯ä¿æŒæ´»è·ƒ
        do {
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
        }
        
        // ç«‹å³æ›´æ–°Live Activity
        updateLiveActivity()
    }
    
    @objc private func handleAppWillEnterForeground() {
        
        // æ›´æ–°Live ActivityçŠ¶æ€
        if isRecording {
            updateLiveActivity()
        }
    }
    
    @objc private func handleAppWillTerminate() {
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œç«‹å³åœæ­¢ï¼ˆä½†æ— æ³•ä¸Šä¼ åˆ°åç«¯ï¼Œå› ä¸ºappå³å°†ç»ˆæ­¢ï¼‰
        if isRecording {
            
            // åŒæ­¥åœæ­¢å½•éŸ³ï¼ˆå› ä¸ºæ—¶é—´ç´§è¿«ï¼‰
            isRecording = false
            isPaused = false
            
            // åœæ­¢å½•éŸ³å™¨
            audioRecorder?.stop()
            recordingTimer?.invalidate()
            
            // åœæ­¢éŸ³é¢‘å¼•æ“
            if audioEngine.isRunning {
                audioEngine.stop()
                audioEngine.inputNode.removeTap(onBus: 0)
            }
            
            recognitionRequest?.endAudio()
            recognitionTask?.cancel()
            recognitionTask = nil
            recognitionRequest = nil
            
            // åŒæ­¥ç»“æŸ Live Activityï¼ˆä½¿ç”¨ä¿¡å·é‡ç­‰å¾…å®Œæˆï¼‰
            if let activity = activity {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: recognizedText,
                    duration: recordingDuration,
                    isRecording: false,
                    isPaused: false
                )
                
                let semaphore = DispatchSemaphore(value: 0)
                Task {
                    // iOS 16.2+ æ¨èä½¿ç”¨ end(_ content:dismissalPolicy:)ï¼›è¿™é‡Œç»Ÿä¸€èµ° ActivityContentï¼Œé¿å…åºŸå¼ƒè­¦å‘Š
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                    semaphore.signal()
                }
                // æœ€å¤šç­‰å¾…0.5ç§’
                _ = semaphore.wait(timeout: .now() + 0.5)
                self.activity = nil
            }
            
        } else {
            // å³ä½¿æ²¡åœ¨å½•éŸ³ï¼Œä¹Ÿè¦æ¸…ç†å¯èƒ½æ®‹ç•™çš„Activity
            endLiveActivityImmediately()
        }
    }
    
    @objc private func handleAudioInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            if isRecording && !isPaused {
                pauseRecording()
            }
            
        case .ended:
            if let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                if options.contains(.shouldResume) && isPaused {
                    resumeRecording()
                }
            }
            
        @unknown default:
            break
        }
    }
    
    @objc private func handlePauseFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.pauseRecording()
        }
    }
    
    @objc private func handleResumeFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.resumeRecording()
        }
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    private func formatDate(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.dateFormat = "MMæœˆddæ—¥ HH:mm"
        return formatter.string(from: date)
    }
    
    private func ensureRecordingsFolder() -> URL {
        // ç»Ÿä¸€åç«¯æ¥å…¥ï¼šå½•éŸ³æ–‡ä»¶ä¸åº”æŒä¹…åŒ–åœ¨ Documentsï¼Œæ”¹ç”¨ä¸´æ—¶ç›®å½•ï¼ˆå¯è¢«ç³»ç»Ÿå›æ”¶ï¼Œä¸”ä¼šåœ¨å¯åŠ¨æ—¶æ¸…ç†ï¼‰ã€‚
        let baseURL = FileManager.default.temporaryDirectory
        let folderURL = baseURL.appendingPathComponent("MeetingRecordings", isDirectory: true)
        
        if !FileManager.default.fileExists(atPath: folderURL.path) {
            do {
                try FileManager.default.createDirectory(at: folderURL, withIntermediateDirectories: true)
            } catch {
            }
        }
        
        return folderURL
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
    }
}

