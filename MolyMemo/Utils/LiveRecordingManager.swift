import Foundation
import AVFoundation
import Speech
import ActivityKit
import SwiftData
import UIKit

// å®æ—¶å½•éŸ³ç®¡ç†å™¨ - åŒæ—¶å½•éŸ³å’Œå®æ—¶è½¬å†™
class LiveRecordingManager: ObservableObject {
    static let shared = LiveRecordingManager()
    
    @Published var isRecording = false
    /// ç”¨æˆ·ç‚¹å‡»â€œå¼€å§‹å½•éŸ³â€ååˆ°çœŸæ­£å¼€å§‹å½•éŸ³ä¹‹é—´çš„è¿‡æ¸¡æ€ï¼Œç”¨æ¥è®© UI ç«‹åˆ»å“åº”ï¼Œé¿å…ä¸»çº¿ç¨‹è¢«åˆå§‹åŒ–å·¥ä½œå¡ä½
    @Published var isStartingRecording = false
    @Published var isPaused = false
    @Published var recognizedText = ""
    @Published var recordingDuration: TimeInterval = 0
    
    private var audioRecorder: AVAudioRecorder?
    private var recordingTimer: Timer?
    private var audioURL: URL?
    
    // Speech è¯†åˆ«å™¨
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // Live Activity
    private var activity: Activity<MeetingRecordingAttributes>?

    // Widget/å¿«æ·æŒ‡ä»¤åœºæ™¯ï¼šå¯ä»¥åªåœ¨åå°åšè½¬å†™ï¼Œä½†ä¸æŠŠæ–‡æœ¬æ¨åˆ° UIï¼ˆçµåŠ¨å²›/Live Activityï¼‰
    private var publishTranscriptionToUI: Bool = true
    // ä¸Šä¼ ç”Ÿæˆåï¼Œæ˜¯å¦å†™å…¥â€œèŠå¤©å®¤ä¼šè®®å¡ç‰‡â€ï¼ˆç”Ÿæˆä¸­ -> å®Œæˆ/å¤±è´¥ï¼‰
    private var uploadToChat: Bool = true
    // ä¸Šä¼ ç”Ÿæˆåï¼Œæ˜¯å¦é€šçŸ¥ä¼šè®®åˆ—è¡¨é¡µæ’å…¥/æ›´æ–°â€œå ä½æ¡ç›®â€
    private var updateMeetingList: Bool = false
    
    // ä¿å­˜ ModelContext çš„å›è°ƒ
    var modelContextProvider: (() -> ModelContext?)?
    
    private init() {
        // ç›‘å¬appçŠ¶æ€å˜åŒ–ï¼Œç¡®ä¿åå°å½•éŸ³æ­£å¸¸
        setupBackgroundHandling()
        // å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
        cleanupStaleActivities()
    }

    private func runOnMain(_ block: @escaping () -> Void) {
        if Thread.isMainThread { block() } else { DispatchQueue.main.async(execute: block) }
    }

    private func hasMicrophonePermission() -> Bool {
        if #available(iOS 17.0, *) {
            return AVAudioApplication.shared.recordPermission == .granted
        } else {
            return AVAudioSession.sharedInstance().recordPermission == .granted
        }
    }

    private func hasSpeechPermission() -> Bool {
        SFSpeechRecognizer.authorizationStatus() == .authorized
    }
    
    // å¼€å§‹å½•éŸ³
    /// - Parameter publishTranscriptionToUI: æ˜¯å¦åœ¨ Live Activity / çµåŠ¨å²›æ˜¾ç¤ºå®æ—¶è½¬å†™æ–‡æœ¬ï¼ˆé»˜è®¤ trueï¼‰ã€‚
    /// - Parameter uploadToChat: æ˜¯å¦åœ¨èŠå¤©å®¤ç”Ÿæˆä¼šè®®å¡ç‰‡ï¼ˆé»˜è®¤ trueï¼‰ã€‚
    /// - Parameter updateMeetingList: æ˜¯å¦åœ¨ä¼šè®®åˆ—è¡¨é¡µæ’å…¥/æ›´æ–°å ä½æ¡ç›®ï¼ˆé»˜è®¤ falseï¼‰ã€‚
    func startRecording(publishTranscriptionToUI: Bool = true, uploadToChat: Bool = true, updateMeetingList: Bool = false) {
        self.publishTranscriptionToUI = publishTranscriptionToUI
        self.uploadToChat = uploadToChat
        self.updateMeetingList = updateMeetingList
        print("[RecordingFlow] ğŸ™ï¸ startRecording publishToUI=\(publishTranscriptionToUI)")

        // å…ˆè®© UI è¿›å…¥â€œå¯åŠ¨ä¸­â€ï¼Œé¿å…ç”¨æˆ·æ„Ÿè§‰ç‚¹äº†æ²¡ååº”
        runOnMain { [weak self] in
            guard let self else { return }
            if self.isRecording { return }
            self.isStartingRecording = true
        }
        
        // å¿«è·¯å¾„ï¼šæƒé™éƒ½å·²æœ‰æ—¶ï¼Œä¸èµ°ç³»ç»Ÿå¼¹çª—é“¾è·¯ï¼Œç›´æ¥å¼€å§‹åˆå§‹åŒ–
        if hasMicrophonePermission(), hasSpeechPermission() {
            setupRecordingAsync()
            return
        }

        // è¯·æ±‚æƒé™ï¼ˆå¯èƒ½è§¦å‘ç³»ç»Ÿå¼¹çª—ï¼‰
        requestPermissions { [weak self] granted in
            guard let self else { return }
            guard granted else {
                print("[RecordingFlow] âŒ startRecording permission denied")
                self.runOnMain { self.isStartingRecording = false }
                return
            }
            self.setupRecordingAsync()
        }
    }
    
    private func requestPermissions(completion: @escaping (Bool) -> Void) {
        // è¯·æ±‚éº¦å…‹é£æƒé™ï¼ˆiOS 17 åŠä»¥ä¸Šä½¿ç”¨ AVAudioApplicationï¼‰
        let requestMicPermission: (@escaping (Bool) -> Void) -> Void = { handler in
            if #available(iOS 17.0, *) {
                AVAudioApplication.requestRecordPermission { granted in
                    handler(granted)
                }
            } else {
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    handler(granted)
                }
            }
        }

        requestMicPermission { micGranted in
            guard micGranted else {
                completion(false)
                return
            }

            // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    completion(authStatus == .authorized)
                }
            }
        }
    }
    
    /// æŠŠè€—æ—¶åˆå§‹åŒ–å°½é‡æŒªåˆ°åå°ï¼Œä¸»çº¿ç¨‹åªåšâ€œç«‹åˆ»åˆ·æ–° UIâ€
    private func setupRecordingAsync() {
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self else { return }
            self.setupRecordingInBackground()
        }
    }

    private func setupRecordingInBackground() {
        // é…ç½®éŸ³é¢‘ä¼šè¯ - æ”¯æŒåå°å½•éŸ³ï¼ˆæ”¾åå°ï¼Œé¿å…ä¸»çº¿ç¨‹å¡é¡¿ï¼‰
        let audioSession = AVAudioSession.sharedInstance()
        do {
            let options: AVAudioSession.CategoryOptions = [
                .defaultToSpeaker,
                .allowBluetoothA2DP,
                .mixWithOthers
            ]
            try audioSession.setCategory(.playAndRecord, mode: .default, options: options)
            try audioSession.setActive(true)
        } catch {
            print("[RecordingFlow] âŒ setupRecording audioSession failed -> \(error.localizedDescription)")
            runOnMain { [weak self] in self?.isStartingRecording = false }
            return
        }

        // å‡†å¤‡å½•éŸ³æ–‡ä»¶ï¼ˆç»Ÿä¸€å­˜æ”¾åœ¨ MeetingRecordings æ–‡ä»¶å¤¹ï¼‰
        let recordingsFolder = ensureRecordingsFolder()
        let newAudioURL = recordingsFolder.appendingPathComponent("meeting_\(Int(Date().timeIntervalSince1970)).m4a")
        print("[RecordingFlow] ğŸ“ recording file = \(newAudioURL.path)")

        // é…ç½®å½•éŸ³è®¾ç½®ï¼ˆm4a AAC æ ¼å¼ï¼Œé«˜è´¨é‡å‹ç¼©ï¼‰
        let settings: [String: Any] = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 44100.0,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue,
            AVEncoderBitRateKey: 128000
        ]

        do {
            let recorder = try AVAudioRecorder(url: newAudioURL, settings: settings)
            recorder.record()
            print("[RecordingFlow] âœ… AVAudioRecorder started (m4a/AAC 44.1k 1ch)")

            // ä¸»çº¿ç¨‹ï¼šç«‹åˆ»åˆ·æ–° UIï¼ˆæŒ‰é’®/åˆ—è¡¨ï¼‰
            runOnMain { [weak self] in
                guard let self else { return }
                self.audioURL = newAudioURL
                self.audioRecorder = recorder
                self.isRecording = true
                self.isStartingRecording = false
                self.isPaused = false
                self.recognizedText = ""
                self.recordingDuration = 0

                // å¯åŠ¨è®¡æ—¶å™¨æ”¾ä¸»çº¿ç¨‹ runloopï¼Œé¿å…åå° runloop ä¸è¿è¡Œ
                self.recordingTimer?.invalidate()
                self.recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
                    guard let self else { return }
                    self.recordingDuration += 0.5
                }
                RunLoop.main.add(self.recordingTimer!, forMode: .common)
            }

            // è¯­éŸ³è¯†åˆ«/Live Activity å»¶åå¯åŠ¨ï¼Œä¼˜å…ˆè®© UI å…ˆâ€œåŠ¨èµ·æ¥â€
            DispatchQueue.global(qos: .utility).asyncAfter(deadline: .now() + 0.15) { [weak self] in
                self?.startSpeechRecognition()
                self?.startLiveActivity()
            }
        } catch {
            print("[RecordingFlow] âŒ AVAudioRecorder create/start failed -> \(error.localizedDescription)")
            runOnMain { [weak self] in self?.isStartingRecording = false }
        }
    }
    
    // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
    private func startSpeechRecognition() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        // é…ç½®éŸ³é¢‘å¼•æ“
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    self.updateLiveActivity()
                }
            }
            
            if let error = error {
                let nsError = error as NSError
                if nsError.code != 301 {  // å¿½ç•¥å–æ¶ˆé”™è¯¯
                }
            }
        }
    }
    
    // æš‚åœå½•éŸ³
    func pauseRecording() {
        guard isRecording && !isPaused else { return }
        
        isPaused = true
        print("[RecordingFlow] â¸ï¸ pauseRecording")
        
        // æš‚åœå½•éŸ³å™¨
        audioRecorder?.pause()
        recordingTimer?.invalidate()
        
        // æš‚åœéŸ³é¢‘å¼•æ“
        audioEngine.pause()
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // ç»§ç»­å½•éŸ³
    func resumeRecording() {
        guard isRecording && isPaused else { return }
        
        isPaused = false
        print("[RecordingFlow] â–¶ï¸ resumeRecording")
        
        // ç»§ç»­å½•éŸ³å™¨
        audioRecorder?.record()
        
        // é‡æ–°å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
        recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            self.recordingDuration += 0.5
            self.updateLiveActivity()
        }
        RunLoop.current.add(recordingTimer!, forMode: .common)
        
        // ç»§ç»­éŸ³é¢‘å¼•æ“
        do {
            try audioEngine.start()
        } catch {
        }
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // åœæ­¢å½•éŸ³
    func stopRecording(modelContext: ModelContext? = nil) {
        // SwiftUI/ObservableObject çš„çŠ¶æ€æ›´æ–°å¿…é¡»åœ¨ä¸»çº¿ç¨‹
        if !Thread.isMainThread {
            DispatchQueue.main.async { [weak self] in
                self?.stopRecording(modelContext: modelContext)
            }
            return
        }
        
        guard isRecording else { 
            return 
        }
        // æ³¨æ„ï¼šrecordingDuration æ˜¯ UI è®¡æ—¶å™¨é©±åŠ¨ï¼ˆ0.5s ä¸€è·³ï¼‰ï¼Œç”¨äºå±•ç¤ºå³å¯ï¼›
        // çœŸå®æ—¶é•¿ä»¥ AVAudioRecorder.currentTime ä¸ºå‡†ï¼Œé¿å…çŸ­å½•éŸ³è¢«è¯¯åˆ¤ä¸º 0/è¿‡çŸ­å¯¼è‡´ç›´æ¥ä¸¢å¼ƒï¼Œä»è€Œâ€œæ²¡æœ‰ç”Ÿæˆå¡ç‰‡â€ã€‚
        let recorderSeconds = audioRecorder?.currentTime ?? 0
        let measuredDuration = max(recorderSeconds, recordingDuration)
        print("[RecordingFlow] ğŸ›‘ stopRecording duration=\(measuredDuration)s (ui=\(recordingDuration)s, rec=\(recorderSeconds)s) recognizedTextLen=\(recognizedText.count)")

        // âš¡ï¸ å…³é”®ï¼šä¸»çº¿ç¨‹åªåšâ€œç«‹åˆ»åˆ‡ UI + ç«‹åˆ»å‘é€šçŸ¥â€ï¼Œé‡æ¸…ç†æ”¾åå°ï¼Œé¿å…åœæ­¢æŒ‰é’®ç‚¹å‡»åå¡é¡¿
        let finalDuration = measuredDuration
        let finalAudioURL = audioURL

        isStartingRecording = false
        isRecording = false
        isPaused = false

        // å…ˆåœå½•éŸ³å™¨å¹¶ç»ˆæ­¢è®¡æ—¶ï¼ˆå°½å¿« flush æ–‡ä»¶ï¼Œç¡®ä¿åç»­è¯»å–å®Œæ•´ï¼‰
        audioRecorder?.stop()
        recordingTimer?.invalidate()

        // âœ… ç«‹åˆ»é€šçŸ¥ä¸» App è¿›å…¥â€œä¸Šä¼ /ç”Ÿæˆâ€æµç¨‹ï¼š
        // - ä¸å†ç”¨â€œæ—¶é•¿é˜ˆå€¼â€å†³å®šè¦ä¸è¦ç”Ÿæˆå¡ç‰‡ï¼ˆç”¨æˆ·åœæ­¢å°±åº”ç”Ÿæˆï¼šæˆåŠŸ/å¤±è´¥éƒ½è¦ç»™ç»“æœï¼‰
        // - è‹¥æ–‡ä»¶ç¼ºå¤±/æ— æ•ˆï¼Œä¹Ÿä¼šå‘é€šçŸ¥ï¼Œè®©ä¸Šå±‚ç”Ÿæˆâ€œå¤±è´¥å¡ç‰‡â€è€Œä¸æ˜¯é™é»˜æ¶ˆå¤±
        postRecordingNeedsUpload(audioURL: finalAudioURL, duration: finalDuration)

        // åå°åšè€—æ—¶æ¸…ç†ï¼Œé¿å…é˜»å¡ UI
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.cleanupAfterStop()
            DispatchQueue.main.async {
                // ç»“æŸ Live Activityï¼ˆå†…éƒ¨åŒ…å«å±•ç¤ºä¸å»¶è¿Ÿé€»è¾‘ï¼‰
                self?.endLiveActivity()
            }
        }
    }

    /// åœæ­¢å½•éŸ³åçš„èµ„æºæ¸…ç†ï¼ˆæ”¾åå°æ‰§è¡Œï¼Œé¿å… UI å¡é¡¿ï¼‰
    private func cleanupAfterStop() {
        // åœæ­¢éŸ³é¢‘å¼•æ“ï¼ˆè¯­éŸ³è¯†åˆ«ç”¨ï¼‰
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }

        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        // æ”¶å› AudioSessionï¼Œé¿å…åç»­æ’­æ”¾å¼‚å¸¸
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: [.notifyOthersOnDeactivation])
        } catch {
            print("[RecordingFlow] âš ï¸ audioSession deactivate failed -> \(error.localizedDescription)")
        }
    }
    
    /// é€šçŸ¥ä¸»Appä¸Šä¼ éŸ³é¢‘åˆ°åç«¯ç”Ÿæˆä¼šè®®çºªè¦
    /// æ³¨æ„ï¼šè¿™é‡Œåªå‘é€é€šçŸ¥ï¼Œå®é™…çš„åç«¯è°ƒç”¨ç”±ä¸»Appå¤„ç†ï¼ˆå› ä¸ºWidget Extensionæ— æ³•è®¿é—®MeetingMinutesServiceï¼‰
    private func uploadToBackend() {
        postRecordingNeedsUpload(audioURL: audioURL, duration: recordingDuration)
    }

    private func postRecordingNeedsUpload(audioURL: URL?, duration: TimeInterval) {
        let audioPath = audioURL?.path ?? ""
        print("[RecordingFlow] â˜ï¸ notify backend upload audioPath=\(audioPath)")

        let title = "Molyå½•éŸ³ - \(formatDate(Date()))"
        let date = Date()

        let meetingData: [String: Any] = [
            "title": title,
            "date": date,
            "duration": duration,
            "audioPath": audioPath,
            "needsBackendUpload": true,
            // æ–°å­—æ®µï¼ˆæ›´æ¸…æ™°ï¼‰
            "uploadToChat": uploadToChat,
            "updateMeetingList": updateMeetingList,
            // æ—§å­—æ®µï¼šç»§ç»­å†™å…¥ä»¥å…¼å®¹å†å²ç›‘å¬é€»è¾‘ï¼ˆå«å¤–éƒ¨ Extension/è€ç‰ˆæœ¬ï¼‰
            "suppressChatCard": !uploadToChat
        ]

        // NotificationCenter çš„ publisher é»˜è®¤åœ¨â€œå‘é€çº¿ç¨‹â€å›è°ƒï¼›ä¸ºäº†é¿å… SwiftUI çŠ¶æ€åœ¨åå°æ›´æ–°ï¼Œå¼ºåˆ¶åœ¨ä¸»çº¿ç¨‹å‘é€
        if Thread.isMainThread {
            NotificationCenter.default.post(
                name: NSNotification.Name("RecordingNeedsUpload"),
                object: nil,
                userInfo: meetingData
            )
        } else {
            DispatchQueue.main.async {
                NotificationCenter.default.post(
                    name: NSNotification.Name("RecordingNeedsUpload"),
                    object: nil,
                    userInfo: meetingData
                )
            }
        }
    }
    
    // MARK: - Live Activity ç®¡ç†
    
    private func startLiveActivity() {
        guard ActivityAuthorizationInfo().areActivitiesEnabled else {
            return
        }
        
        let attributes = MeetingRecordingAttributes(meetingTitle: "Molyå½•éŸ³")
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: isPaused ? "å·²æš‚åœ" : "å½•éŸ³ä¸­",
            duration: 0,
            isRecording: true,
            isPaused: false
        )
        
        do {
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // æœ€é«˜ä¼˜å…ˆçº§ï¼Œä¿æŒå±•å¼€çŠ¶æ€
            )
            
            activity = try Activity<MeetingRecordingAttributes>.request(
                attributes: attributes,
                content: activityContent,
                pushType: nil
            )
        } catch {
        }
    }
    
    private func updateLiveActivity() {
        guard let activity = activity else { return }
        
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: {
                // çµåŠ¨å²›ä¸å†å±•ç¤ºå®æ—¶è½¬å†™/è®¡æ—¶ï¼Œå›ºå®šæ–‡æ¡ˆå³å¯
                return isPaused ? "å·²æš‚åœ" : "å½•éŸ³ä¸­"
            }(),
            duration: recordingDuration,
            isRecording: isRecording,
            isPaused: isPaused
        )
        
        Task { @MainActor in
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // ä¿æŒæœ€é«˜ä¼˜å…ˆçº§
            )
            await activity.update(activityContent)
        }
    }
    
    private func endLiveActivity() {
        guard let activity = activity else { return }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: "å½•éŸ³ä¸­",
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        // æ•è·å½“å‰çš„ activity å¼•ç”¨
        let currentActivity = activity
        
        Task {
            // ç‚¹å‡»åœæ­¢åç«‹åˆ»ç»“æŸçµåŠ¨å²›/Live Activityï¼ˆä¸å†åœç•™/ä¸å±•ç¤ºè®¡æ—¶å®Œæˆæ€ï¼‰
            if #available(iOS 16.2, *) {
                let content = ActivityContent(
                    state: finalState,
                    staleDate: nil,
                    relevanceScore: 100.0
                )
                await currentActivity.end(content, dismissalPolicy: .immediate)
            } else {
                await currentActivity.end(dismissalPolicy: .immediate)
            }
        }
        
        // ç½®ç©ºå®ä¾‹ï¼Œé˜²æ­¢é‡å¤æ“ä½œ
        self.activity = nil
    }
    
    // ç«‹å³å¼ºåˆ¶ç»“æŸLive Activityï¼ˆç”¨äºAppç»ˆæ­¢æ—¶ï¼‰
    private func endLiveActivityImmediately() {
        guard let activity = activity else { 
            // æ²¡æœ‰activityå®ä¾‹ï¼Œå°è¯•æ¸…ç†æ‰€æœ‰æ´»åŠ¨çš„Activity
            cleanupStaleActivities()
            return
        }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText,
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        let semaphore = DispatchSemaphore(value: 0)
        Task {
            if #available(iOS 16.2, *) {
                let content = ActivityContent(
                    state: finalState,
                    staleDate: nil,
                    relevanceScore: 100.0
                )
                await activity.end(content, dismissalPolicy: .immediate)
            } else {
                await activity.end(dismissalPolicy: .immediate)
            }
            semaphore.signal()
        }
        _ = semaphore.wait(timeout: .now() + 0.5)
        self.activity = nil
    }
    
    // æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
    private func cleanupStaleActivities() {
        
        Task { @MainActor in
            let activities = Activity<MeetingRecordingAttributes>.activities
            guard !activities.isEmpty else {
                return
            }
            
            for activity in activities {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: "",
                    duration: 0,
                    isRecording: false,
                    isPaused: false
                )
                if #available(iOS 16.2, *) {
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                } else {
                    await activity.end(dismissalPolicy: .immediate)
                }
            }
        }
    }
    
    // MARK: - åå°å¤„ç†
    
    private func setupBackgroundHandling() {
        // ç›‘å¬appè¿›å…¥åå°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppDidEnterBackground),
            name: UIApplication.didEnterBackgroundNotification,
            object: nil
        )
        
        // ç›‘å¬appè¿›å…¥å‰å°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillEnterForeground),
            name: UIApplication.willEnterForegroundNotification,
            object: nil
        )
        
        // ç›‘å¬appå³å°†ç»ˆæ­¢
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillTerminate),
            name: UIApplication.willTerminateNotification,
            object: nil
        )
        
        // ç›‘å¬éŸ³é¢‘ä¸­æ–­
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioInterruption),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„æš‚åœå‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handlePauseFromWidget),
            name: NSNotification.Name("PauseRecordingFromWidget"),
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„ç»§ç»­å‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleResumeFromWidget),
            name: NSNotification.Name("ResumeRecordingFromWidget"),
            object: nil
        )
    }
    
    @objc private func handleAppDidEnterBackground() {
        
        guard isRecording else { return }
        
        // ç¡®ä¿éŸ³é¢‘ä¼šè¯ä¿æŒæ´»è·ƒ
        do {
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
        }
        
        // ç«‹å³æ›´æ–°Live Activity
        updateLiveActivity()
    }
    
    @objc private func handleAppWillEnterForeground() {
        
        // æ›´æ–°Live ActivityçŠ¶æ€
        if isRecording {
            updateLiveActivity()
        }
    }
    
    @objc private func handleAppWillTerminate() {
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œç«‹å³åœæ­¢ï¼ˆä½†æ— æ³•ä¸Šä¼ åˆ°åç«¯ï¼Œå› ä¸ºappå³å°†ç»ˆæ­¢ï¼‰
        if isRecording {
            
            // åŒæ­¥åœæ­¢å½•éŸ³ï¼ˆå› ä¸ºæ—¶é—´ç´§è¿«ï¼‰
            isRecording = false
            isPaused = false
            
            // åœæ­¢å½•éŸ³å™¨
            audioRecorder?.stop()
            recordingTimer?.invalidate()
            
            // åœæ­¢éŸ³é¢‘å¼•æ“
            if audioEngine.isRunning {
                audioEngine.stop()
                audioEngine.inputNode.removeTap(onBus: 0)
            }
            
            recognitionRequest?.endAudio()
            recognitionTask?.cancel()
            recognitionTask = nil
            recognitionRequest = nil
            
            // åŒæ­¥ç»“æŸ Live Activityï¼ˆä½¿ç”¨ä¿¡å·é‡ç­‰å¾…å®Œæˆï¼‰
            if let activity = activity {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: recognizedText,
                    duration: recordingDuration,
                    isRecording: false,
                    isPaused: false
                )
                
                let semaphore = DispatchSemaphore(value: 0)
                Task {
                    // iOS 16.2+ æ¨èä½¿ç”¨ end(_ content:dismissalPolicy:)ï¼›è¿™é‡Œç»Ÿä¸€èµ° ActivityContentï¼Œé¿å…åºŸå¼ƒè­¦å‘Š
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                    semaphore.signal()
                }
                // æœ€å¤šç­‰å¾…0.5ç§’
                _ = semaphore.wait(timeout: .now() + 0.5)
                self.activity = nil
            }
            
        } else {
            // å³ä½¿æ²¡åœ¨å½•éŸ³ï¼Œä¹Ÿè¦æ¸…ç†å¯èƒ½æ®‹ç•™çš„Activity
            endLiveActivityImmediately()
        }
    }
    
    @objc private func handleAudioInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            if isRecording && !isPaused {
                pauseRecording()
            }
            
        case .ended:
            if let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                if options.contains(.shouldResume) && isPaused {
                    resumeRecording()
                }
            }
            
        @unknown default:
            break
        }
    }
    
    @objc private func handlePauseFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.pauseRecording()
        }
    }
    
    @objc private func handleResumeFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.resumeRecording()
        }
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    private func formatDate(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.dateFormat = "MMæœˆddæ—¥ HH:mm"
        return formatter.string(from: date)
    }
    
    private func ensureRecordingsFolder() -> URL {
        // ç»Ÿä¸€åç«¯æ¥å…¥ï¼šå½•éŸ³æ–‡ä»¶ä¸åº”æŒä¹…åŒ–åœ¨ Documentsï¼Œæ”¹ç”¨ä¸´æ—¶ç›®å½•ï¼ˆå¯è¢«ç³»ç»Ÿå›æ”¶ï¼Œä¸”ä¼šåœ¨å¯åŠ¨æ—¶æ¸…ç†ï¼‰ã€‚
        let baseURL = FileManager.default.temporaryDirectory
        let folderURL = baseURL.appendingPathComponent("MeetingRecordings", isDirectory: true)
        
        if !FileManager.default.fileExists(atPath: folderURL.path) {
            do {
                try FileManager.default.createDirectory(at: folderURL, withIntermediateDirectories: true)
            } catch {
            }
        }
        
        return folderURL
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
    }
}

