import Foundation
import AVFoundation
import Speech
import ActivityKit
import SwiftData
import UIKit

// å®æ—¶å½•éŸ³ç®¡ç†å™¨ - åŒæ—¶å½•éŸ³å’Œå®æ—¶è½¬å†™
class LiveRecordingManager: ObservableObject {
    static let shared = LiveRecordingManager()
    
    @Published var isRecording = false
    @Published var isPaused = false
    @Published var recognizedText = ""
    @Published var recordingDuration: TimeInterval = 0
    
    private var audioRecorder: AVAudioRecorder?
    private var recordingTimer: Timer?
    private var audioURL: URL?
    
    // Speech è¯†åˆ«å™¨
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // Live Activity
    private var activity: Activity<MeetingRecordingAttributes>?

    // Widget/å¿«æ·æŒ‡ä»¤åœºæ™¯ï¼šå¯ä»¥åªåœ¨åå°åšè½¬å†™ï¼Œä½†ä¸æŠŠæ–‡æœ¬æ¨åˆ° UIï¼ˆçµåŠ¨å²›/Live Activityï¼‰
    private var publishTranscriptionToUI: Bool = true
    // ä¼šè®®è®°å½•é¡µå†…å‘èµ·çš„å½•éŸ³ï¼šä¸éœ€è¦å¾€èŠå¤©å®¤æ’å…¥â€œç”Ÿæˆä¸­å¡ç‰‡â€
    private var suppressChatCardOnUpload: Bool = false
    
    // ä¿å­˜ ModelContext çš„å›è°ƒ
    var modelContextProvider: (() -> ModelContext?)?
    
    private init() {
        // ç›‘å¬appçŠ¶æ€å˜åŒ–ï¼Œç¡®ä¿åå°å½•éŸ³æ­£å¸¸
        setupBackgroundHandling()
        // å¯åŠ¨æ—¶æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
        cleanupStaleActivities()
    }
    
    // å¼€å§‹å½•éŸ³
    /// - Parameter publishTranscriptionToUI: æ˜¯å¦åœ¨ Live Activity / çµåŠ¨å²›æ˜¾ç¤ºå®æ—¶è½¬å†™æ–‡æœ¬ï¼ˆé»˜è®¤ trueï¼‰ã€‚
    /// - Parameter suppressChatCardOnUpload: ä»…ä¼šè®®è®°å½•é¡µä½¿ç”¨ï¼šä¸Šä¼ ç”Ÿæˆæ—¶ä¸æ›´æ–°èŠå¤©å®¤ï¼ˆé»˜è®¤ falseï¼‰ã€‚
    func startRecording(publishTranscriptionToUI: Bool = true, suppressChatCardOnUpload: Bool = false) {
        self.publishTranscriptionToUI = publishTranscriptionToUI
        self.suppressChatCardOnUpload = suppressChatCardOnUpload
        print("[RecordingFlow] ğŸ™ï¸ startRecording publishToUI=\(publishTranscriptionToUI)")
        
        // è¯·æ±‚æƒé™
        requestPermissions { [weak self] granted in
            guard granted else {
                print("[RecordingFlow] âŒ startRecording permission denied")
                return
            }
            
            self?.setupRecording()
        }
    }
    
    private func requestPermissions(completion: @escaping (Bool) -> Void) {
        // è¯·æ±‚éº¦å…‹é£æƒé™ï¼ˆiOS 17 åŠä»¥ä¸Šä½¿ç”¨ AVAudioApplicationï¼‰
        let requestMicPermission: (@escaping (Bool) -> Void) -> Void = { handler in
            if #available(iOS 17.0, *) {
                AVAudioApplication.requestRecordPermission { granted in
                    handler(granted)
                }
            } else {
                AVAudioSession.sharedInstance().requestRecordPermission { granted in
                    handler(granted)
                }
            }
        }

        requestMicPermission { micGranted in
            guard micGranted else {
                completion(false)
                return
            }

            // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    completion(authStatus == .authorized)
                }
            }
        }
    }
    
    private func setupRecording() {
        // é…ç½®éŸ³é¢‘ä¼šè¯ - æ”¯æŒåå°å½•éŸ³
        let audioSession = AVAudioSession.sharedInstance()
        do {
            let options: AVAudioSession.CategoryOptions = [
                .defaultToSpeaker,
                .allowBluetoothA2DP,
                .mixWithOthers
            ]
            try audioSession.setCategory(.playAndRecord, mode: .default, options: options)
            try audioSession.setActive(true)
        } catch {
            print("[RecordingFlow] âŒ setupRecording audioSession failed -> \(error.localizedDescription)")
            return
        }
        
        // å‡†å¤‡å½•éŸ³æ–‡ä»¶ï¼ˆç»Ÿä¸€å­˜æ”¾åœ¨ MeetingRecordings æ–‡ä»¶å¤¹ï¼‰
        let recordingsFolder = ensureRecordingsFolder()
        audioURL = recordingsFolder.appendingPathComponent("meeting_\(Int(Date().timeIntervalSince1970)).m4a")
        
        guard let audioURL = audioURL else { return }
        print("[RecordingFlow] ğŸ“ recording file = \(audioURL.path)")
        
        // é…ç½®å½•éŸ³è®¾ç½®ï¼ˆm4a AAC æ ¼å¼ï¼Œé«˜è´¨é‡å‹ç¼©ï¼‰
        let settings: [String: Any] = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 44100.0,
            AVNumberOfChannelsKey: 1,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue,
            AVEncoderBitRateKey: 128000
        ]
        
        do {
            // åˆ›å»ºå½•éŸ³å™¨
            audioRecorder = try AVAudioRecorder(url: audioURL, settings: settings)
            audioRecorder?.record()
            print("[RecordingFlow] âœ… AVAudioRecorder started (m4a/AAC 44.1k 1ch)")
            
            // é‡ç½®çŠ¶æ€
            isRecording = true
            recognizedText = ""
            recordingDuration = 0
            
            // å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
            recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
                guard let self = self else { return }
                self.recordingDuration += 0.5
                // ä¸å†ç”¨çµåŠ¨å²›å±•ç¤ºè®¡æ—¶ï¼šé¿å…é¢‘ç¹åˆ·æ–° Live Activity
            }
            RunLoop.current.add(recordingTimer!, forMode: .common)
            
            // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
            startSpeechRecognition()
            
            // å¯åŠ¨ Live Activity
            startLiveActivity()
            
        } catch {
            print("[RecordingFlow] âŒ AVAudioRecorder create/start failed -> \(error.localizedDescription)")
        }
    }
    
    // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
    private func startSpeechRecognition() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        // é…ç½®éŸ³é¢‘å¼•æ“
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
        } catch {
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    self.updateLiveActivity()
                }
            }
            
            if let error = error {
                let nsError = error as NSError
                if nsError.code != 301 {  // å¿½ç•¥å–æ¶ˆé”™è¯¯
                }
            }
        }
    }
    
    // æš‚åœå½•éŸ³
    func pauseRecording() {
        guard isRecording && !isPaused else { return }
        
        isPaused = true
        print("[RecordingFlow] â¸ï¸ pauseRecording")
        
        // æš‚åœå½•éŸ³å™¨
        audioRecorder?.pause()
        recordingTimer?.invalidate()
        
        // æš‚åœéŸ³é¢‘å¼•æ“
        audioEngine.pause()
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // ç»§ç»­å½•éŸ³
    func resumeRecording() {
        guard isRecording && isPaused else { return }
        
        isPaused = false
        print("[RecordingFlow] â–¶ï¸ resumeRecording")
        
        // ç»§ç»­å½•éŸ³å™¨
        audioRecorder?.record()
        
        // é‡æ–°å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
        recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            self.recordingDuration += 0.5
            self.updateLiveActivity()
        }
        RunLoop.current.add(recordingTimer!, forMode: .common)
        
        // ç»§ç»­éŸ³é¢‘å¼•æ“
        do {
            try audioEngine.start()
        } catch {
        }
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
    }
    
    // åœæ­¢å½•éŸ³
    func stopRecording(modelContext: ModelContext? = nil) {
        // SwiftUI/ObservableObject çš„çŠ¶æ€æ›´æ–°å¿…é¡»åœ¨ä¸»çº¿ç¨‹
        if !Thread.isMainThread {
            DispatchQueue.main.async { [weak self] in
                self?.stopRecording(modelContext: modelContext)
            }
            return
        }
        
        guard isRecording else { 
            return 
        }
        print("[RecordingFlow] ğŸ›‘ stopRecording duration=\(recordingDuration)s recognizedTextLen=\(recognizedText.count)")

        // âš¡ï¸ å…³é”®ï¼šä¸»çº¿ç¨‹åªåšâ€œç«‹åˆ»åˆ‡ UI + ç«‹åˆ»å‘é€šçŸ¥â€ï¼Œé‡æ¸…ç†æ”¾åå°ï¼Œé¿å…åœæ­¢æŒ‰é’®ç‚¹å‡»åå¡é¡¿
        let finalDuration = recordingDuration
        let finalAudioURL = audioURL

        isRecording = false
        isPaused = false

        // å…ˆåœå½•éŸ³å™¨å¹¶ç»ˆæ­¢è®¡æ—¶ï¼ˆå°½å¿« flush æ–‡ä»¶ï¼Œç¡®ä¿åç»­è¯»å–å®Œæ•´ï¼‰
        audioRecorder?.stop()
        recordingTimer?.invalidate()

        // å¦‚æœå½•éŸ³æ—¶é—´å¤ªçŸ­ï¼ˆå°äº 2 ç§’ï¼‰ï¼Œåˆ™ç›´æ¥ä¸¢å¼ƒï¼ˆä½†æ¸…ç†ä»æ”¾åå°ï¼‰
        if finalDuration < 2.0 {
            print("[RecordingFlow] âš ï¸ Recording too short (\(finalDuration)s), discarding.")
            if let url = finalAudioURL {
                try? FileManager.default.removeItem(at: url)
            }
            DispatchQueue.main.async { [weak self] in
                self?.endLiveActivity()
            }
            // åå°æ¸…ç†è¯­éŸ³è¯†åˆ« / AudioSession
            DispatchQueue.global(qos: .userInitiated).async { [weak self] in
                self?.cleanupAfterStop()
            }
            return
        }

        // âœ… ç«‹åˆ»é€šçŸ¥ä¸» App è¿›å…¥â€œä¸Šä¼ /ç”Ÿæˆâ€æµç¨‹ï¼ˆMeetingRecordView ä¼šå³æ—¶æ’å…¥ loading å°å¡ç‰‡ï¼‰
        if let url = finalAudioURL {
            postRecordingNeedsUpload(audioURL: url, duration: finalDuration)
        }

        // åå°åšè€—æ—¶æ¸…ç†ï¼Œé¿å…é˜»å¡ UI
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.cleanupAfterStop()
            DispatchQueue.main.async {
                // ç»“æŸ Live Activityï¼ˆå†…éƒ¨åŒ…å«å±•ç¤ºä¸å»¶è¿Ÿé€»è¾‘ï¼‰
                self?.endLiveActivity()
            }
        }
    }

    /// åœæ­¢å½•éŸ³åçš„èµ„æºæ¸…ç†ï¼ˆæ”¾åå°æ‰§è¡Œï¼Œé¿å… UI å¡é¡¿ï¼‰
    private func cleanupAfterStop() {
        // åœæ­¢éŸ³é¢‘å¼•æ“ï¼ˆè¯­éŸ³è¯†åˆ«ç”¨ï¼‰
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }

        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil

        // æ”¶å› AudioSessionï¼Œé¿å…åç»­æ’­æ”¾å¼‚å¸¸
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: [.notifyOthersOnDeactivation])
        } catch {
            print("[RecordingFlow] âš ï¸ audioSession deactivate failed -> \(error.localizedDescription)")
        }
    }
    
    /// é€šçŸ¥ä¸»Appä¸Šä¼ éŸ³é¢‘åˆ°åç«¯ç”Ÿæˆä¼šè®®çºªè¦
    /// æ³¨æ„ï¼šè¿™é‡Œåªå‘é€é€šçŸ¥ï¼Œå®é™…çš„åç«¯è°ƒç”¨ç”±ä¸»Appå¤„ç†ï¼ˆå› ä¸ºWidget Extensionæ— æ³•è®¿é—®MeetingMinutesServiceï¼‰
    private func uploadToBackend() {
        guard let audioURL = audioURL else {
            return
        }
        postRecordingNeedsUpload(audioURL: audioURL, duration: recordingDuration)
        
    }

    private func postRecordingNeedsUpload(audioURL: URL, duration: TimeInterval) {
        print("[RecordingFlow] â˜ï¸ notify backend upload audioPath=\(audioURL.path)")

        let title = "Molyå½•éŸ³ - \(formatDate(Date()))"
        let date = Date()
        let audioPath = audioURL.path

        let meetingData: [String: Any] = [
            "title": title,
            "date": date,
            "duration": duration,
            "audioPath": audioPath,
            "needsBackendUpload": true,
            "suppressChatCard": suppressChatCardOnUpload
        ]

        // NotificationCenter çš„ publisher é»˜è®¤åœ¨â€œå‘é€çº¿ç¨‹â€å›è°ƒï¼›ä¸ºäº†é¿å… SwiftUI çŠ¶æ€åœ¨åå°æ›´æ–°ï¼Œå¼ºåˆ¶åœ¨ä¸»çº¿ç¨‹å‘é€
        if Thread.isMainThread {
            NotificationCenter.default.post(
                name: NSNotification.Name("RecordingNeedsUpload"),
                object: nil,
                userInfo: meetingData
            )
        } else {
            DispatchQueue.main.async {
                NotificationCenter.default.post(
                    name: NSNotification.Name("RecordingNeedsUpload"),
                    object: nil,
                    userInfo: meetingData
                )
            }
        }
    }
    
    // MARK: - Live Activity ç®¡ç†
    
    private func startLiveActivity() {
        guard ActivityAuthorizationInfo().areActivitiesEnabled else {
            return
        }
        
        let attributes = MeetingRecordingAttributes(meetingTitle: "Molyå½•éŸ³")
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: isPaused ? "å·²æš‚åœ" : "å½•éŸ³ä¸­",
            duration: 0,
            isRecording: true,
            isPaused: false
        )
        
        do {
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // æœ€é«˜ä¼˜å…ˆçº§ï¼Œä¿æŒå±•å¼€çŠ¶æ€
            )
            
            activity = try Activity<MeetingRecordingAttributes>.request(
                attributes: attributes,
                content: activityContent,
                pushType: nil
            )
        } catch {
        }
    }
    
    private func updateLiveActivity() {
        guard let activity = activity else { return }
        
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: {
                // çµåŠ¨å²›ä¸å†å±•ç¤ºå®æ—¶è½¬å†™/è®¡æ—¶ï¼Œå›ºå®šæ–‡æ¡ˆå³å¯
                return isPaused ? "å·²æš‚åœ" : "å½•éŸ³ä¸­"
            }(),
            duration: recordingDuration,
            isRecording: isRecording,
            isPaused: isPaused
        )
        
        Task { @MainActor in
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // ä¿æŒæœ€é«˜ä¼˜å…ˆçº§
            )
            await activity.update(activityContent)
        }
    }
    
    private func endLiveActivity() {
        guard let activity = activity else { return }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: "å½•éŸ³ä¸­",
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        // æ•è·å½“å‰çš„ activity å¼•ç”¨
        let currentActivity = activity
        
        Task {
            // ç‚¹å‡»åœæ­¢åç«‹åˆ»ç»“æŸçµåŠ¨å²›/Live Activityï¼ˆä¸å†åœç•™/ä¸å±•ç¤ºè®¡æ—¶å®Œæˆæ€ï¼‰
            if #available(iOS 16.2, *) {
                let content = ActivityContent(
                    state: finalState,
                    staleDate: nil,
                    relevanceScore: 100.0
                )
                await currentActivity.end(content, dismissalPolicy: .immediate)
            } else {
                await currentActivity.end(dismissalPolicy: .immediate)
            }
        }
        
        // ç½®ç©ºå®ä¾‹ï¼Œé˜²æ­¢é‡å¤æ“ä½œ
        self.activity = nil
    }
    
    // ç«‹å³å¼ºåˆ¶ç»“æŸLive Activityï¼ˆç”¨äºAppç»ˆæ­¢æ—¶ï¼‰
    private func endLiveActivityImmediately() {
        guard let activity = activity else { 
            // æ²¡æœ‰activityå®ä¾‹ï¼Œå°è¯•æ¸…ç†æ‰€æœ‰æ´»åŠ¨çš„Activity
            cleanupStaleActivities()
            return
        }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText,
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        let semaphore = DispatchSemaphore(value: 0)
        Task {
            if #available(iOS 16.2, *) {
                let content = ActivityContent(
                    state: finalState,
                    staleDate: nil,
                    relevanceScore: 100.0
                )
                await activity.end(content, dismissalPolicy: .immediate)
            } else {
                await activity.end(dismissalPolicy: .immediate)
            }
            semaphore.signal()
        }
        _ = semaphore.wait(timeout: .now() + 0.5)
        self.activity = nil
    }
    
    // æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„Live Activity
    private func cleanupStaleActivities() {
        
        Task { @MainActor in
            let activities = Activity<MeetingRecordingAttributes>.activities
            guard !activities.isEmpty else {
                return
            }
            
            for activity in activities {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: "",
                    duration: 0,
                    isRecording: false,
                    isPaused: false
                )
                if #available(iOS 16.2, *) {
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                } else {
                    await activity.end(dismissalPolicy: .immediate)
                }
            }
        }
    }
    
    // MARK: - åå°å¤„ç†
    
    private func setupBackgroundHandling() {
        // ç›‘å¬appè¿›å…¥åå°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppDidEnterBackground),
            name: UIApplication.didEnterBackgroundNotification,
            object: nil
        )
        
        // ç›‘å¬appè¿›å…¥å‰å°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillEnterForeground),
            name: UIApplication.willEnterForegroundNotification,
            object: nil
        )
        
        // ç›‘å¬appå³å°†ç»ˆæ­¢
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillTerminate),
            name: UIApplication.willTerminateNotification,
            object: nil
        )
        
        // ç›‘å¬éŸ³é¢‘ä¸­æ–­
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioInterruption),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„æš‚åœå‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handlePauseFromWidget),
            name: NSNotification.Name("PauseRecordingFromWidget"),
            object: nil
        )
        
        // ç›‘å¬æ¥è‡ªWidgetçš„ç»§ç»­å‘½ä»¤
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleResumeFromWidget),
            name: NSNotification.Name("ResumeRecordingFromWidget"),
            object: nil
        )
    }
    
    @objc private func handleAppDidEnterBackground() {
        
        guard isRecording else { return }
        
        // ç¡®ä¿éŸ³é¢‘ä¼šè¯ä¿æŒæ´»è·ƒ
        do {
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
        }
        
        // ç«‹å³æ›´æ–°Live Activity
        updateLiveActivity()
    }
    
    @objc private func handleAppWillEnterForeground() {
        
        // æ›´æ–°Live ActivityçŠ¶æ€
        if isRecording {
            updateLiveActivity()
        }
    }
    
    @objc private func handleAppWillTerminate() {
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œç«‹å³åœæ­¢ï¼ˆä½†æ— æ³•ä¸Šä¼ åˆ°åç«¯ï¼Œå› ä¸ºappå³å°†ç»ˆæ­¢ï¼‰
        if isRecording {
            
            // åŒæ­¥åœæ­¢å½•éŸ³ï¼ˆå› ä¸ºæ—¶é—´ç´§è¿«ï¼‰
            isRecording = false
            isPaused = false
            
            // åœæ­¢å½•éŸ³å™¨
            audioRecorder?.stop()
            recordingTimer?.invalidate()
            
            // åœæ­¢éŸ³é¢‘å¼•æ“
            if audioEngine.isRunning {
                audioEngine.stop()
                audioEngine.inputNode.removeTap(onBus: 0)
            }
            
            recognitionRequest?.endAudio()
            recognitionTask?.cancel()
            recognitionTask = nil
            recognitionRequest = nil
            
            // åŒæ­¥ç»“æŸ Live Activityï¼ˆä½¿ç”¨ä¿¡å·é‡ç­‰å¾…å®Œæˆï¼‰
            if let activity = activity {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: recognizedText,
                    duration: recordingDuration,
                    isRecording: false,
                    isPaused: false
                )
                
                let semaphore = DispatchSemaphore(value: 0)
                Task {
                    // iOS 16.2+ æ¨èä½¿ç”¨ end(_ content:dismissalPolicy:)ï¼›è¿™é‡Œç»Ÿä¸€èµ° ActivityContentï¼Œé¿å…åºŸå¼ƒè­¦å‘Š
                    let content = ActivityContent(
                        state: finalState,
                        staleDate: nil,
                        relevanceScore: 100.0
                    )
                    await activity.end(content, dismissalPolicy: .immediate)
                    semaphore.signal()
                }
                // æœ€å¤šç­‰å¾…0.5ç§’
                _ = semaphore.wait(timeout: .now() + 0.5)
                self.activity = nil
            }
            
        } else {
            // å³ä½¿æ²¡åœ¨å½•éŸ³ï¼Œä¹Ÿè¦æ¸…ç†å¯èƒ½æ®‹ç•™çš„Activity
            endLiveActivityImmediately()
        }
    }
    
    @objc private func handleAudioInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            if isRecording && !isPaused {
                pauseRecording()
            }
            
        case .ended:
            if let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                if options.contains(.shouldResume) && isPaused {
                    resumeRecording()
                }
            }
            
        @unknown default:
            break
        }
    }
    
    @objc private func handlePauseFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.pauseRecording()
        }
    }
    
    @objc private func handleResumeFromWidget() {
        DispatchQueue.main.async { [weak self] in
            self?.resumeRecording()
        }
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    private func formatDate(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.dateFormat = "MMæœˆddæ—¥ HH:mm"
        return formatter.string(from: date)
    }
    
    private func ensureRecordingsFolder() -> URL {
        // ç»Ÿä¸€åç«¯æ¥å…¥ï¼šå½•éŸ³æ–‡ä»¶ä¸åº”æŒä¹…åŒ–åœ¨ Documentsï¼Œæ”¹ç”¨ä¸´æ—¶ç›®å½•ï¼ˆå¯è¢«ç³»ç»Ÿå›æ”¶ï¼Œä¸”ä¼šåœ¨å¯åŠ¨æ—¶æ¸…ç†ï¼‰ã€‚
        let baseURL = FileManager.default.temporaryDirectory
        let folderURL = baseURL.appendingPathComponent("MeetingRecordings", isDirectory: true)
        
        if !FileManager.default.fileExists(atPath: folderURL.path) {
            do {
                try FileManager.default.createDirectory(at: folderURL, withIntermediateDirectories: true)
            } catch {
            }
        }
        
        return folderURL
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
    }
}

