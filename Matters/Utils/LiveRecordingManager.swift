import Foundation
import AVFoundation
import Speech
import ActivityKit
import SwiftData
import UIKit

// å®æ—¶å½•éŸ³ç®¡ç†å™¨ - åŒæ—¶å½•éŸ³å’Œå®æ—¶è½¬å†™
class LiveRecordingManager: ObservableObject {
    static let shared = LiveRecordingManager()
    
    @Published var isRecording = false
    @Published var isPaused = false
    @Published var recognizedText = ""
    @Published var recordingDuration: TimeInterval = 0
    
    private var audioRecorder: AVAudioRecorder?
    private var recordingTimer: Timer?
    private var audioURL: URL?
    
    // Speech è¯†åˆ«å™¨
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    // Live Activity
    private var activity: Activity<MeetingRecordingAttributes>?
    
    // ä¿å­˜ ModelContext çš„å›è°ƒ
    var modelContextProvider: (() -> ModelContext?)?
    
    private init() {
        // ç›‘å¬appçŠ¶æ€å˜åŒ–ï¼Œç¡®ä¿åå°å½•éŸ³æ­£å¸¸
        setupBackgroundHandling()
    }
    
    // å¼€å§‹å½•éŸ³
    func startRecording() {
        print("ğŸ¤ å‡†å¤‡å¼€å§‹å½•éŸ³...")
        
        // è¯·æ±‚æƒé™
        requestPermissions { [weak self] granted in
            guard granted else {
                print("âŒ æƒé™è¢«æ‹’ç»")
                return
            }
            
            self?.setupRecording()
        }
    }
    
    private func requestPermissions(completion: @escaping (Bool) -> Void) {
        // è¯·æ±‚éº¦å…‹é£æƒé™
        AVAudioSession.sharedInstance().requestRecordPermission { micGranted in
            guard micGranted else {
                completion(false)
                return
            }
            
            // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
            SFSpeechRecognizer.requestAuthorization { authStatus in
                DispatchQueue.main.async {
                    completion(authStatus == .authorized)
                }
            }
        }
    }
    
    private func setupRecording() {
        // é…ç½®éŸ³é¢‘ä¼šè¯ - æ”¯æŒåå°å½•éŸ³
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth, .mixWithOthers])
            try audioSession.setActive(true)
        } catch {
            print("âŒ éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
            return
        }
        
        // å‡†å¤‡å½•éŸ³æ–‡ä»¶ï¼ˆç»Ÿä¸€å­˜æ”¾åœ¨ MeetingRecordings æ–‡ä»¶å¤¹ï¼‰
        let recordingsFolder = ensureRecordingsFolder()
        audioURL = recordingsFolder.appendingPathComponent("meeting_\(Int(Date().timeIntervalSince1970)).wav")
        
        guard let audioURL = audioURL else { return }
        
        // é…ç½®å½•éŸ³è®¾ç½®ï¼ˆWAV æ ¼å¼ï¼Œä¾¿äºåç»­å¤„ç†ï¼‰
        let settings: [String: Any] = [
            AVFormatIDKey: Int(kAudioFormatLinearPCM),
            AVSampleRateKey: 16000.0,
            AVNumberOfChannelsKey: 1,
            AVLinearPCMBitDepthKey: 16,
            AVLinearPCMIsFloatKey: false,
            AVLinearPCMIsBigEndianKey: false
        ]
        
        do {
            // åˆ›å»ºå½•éŸ³å™¨
            audioRecorder = try AVAudioRecorder(url: audioURL, settings: settings)
            audioRecorder?.record()
            
            // é‡ç½®çŠ¶æ€
            isRecording = true
            recognizedText = ""
            recordingDuration = 0
            
            // å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
            recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
                guard let self = self else { return }
                self.recordingDuration += 0.5
                self.updateLiveActivity()
            }
            RunLoop.current.add(recordingTimer!, forMode: .common)
            
            // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
            startSpeechRecognition()
            
            // å¯åŠ¨ Live Activity
            startLiveActivity()
            
            print("âœ… å½•éŸ³å·²å¯åŠ¨: \(audioURL.lastPathComponent)")
        } catch {
            print("âŒ å½•éŸ³å¯åŠ¨å¤±è´¥: \(error)")
        }
    }
    
    // å¯åŠ¨å®æ—¶è¯­éŸ³è¯†åˆ«
    private func startSpeechRecognition() {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("âŒ è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨")
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("âŒ æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚")
            return
        }
        
        recognitionRequest.shouldReportPartialResults = true
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true
        }
        
        // é…ç½®éŸ³é¢‘å¼•æ“
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            print("âœ… éŸ³é¢‘å¼•æ“å·²å¯åŠ¨")
        } catch {
            print("âŒ å¯åŠ¨éŸ³é¢‘å¼•æ“å¤±è´¥: \(error)")
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            if let result = result {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    self.updateLiveActivity()
                }
            }
            
            if let error = error {
                let nsError = error as NSError
                if nsError.code != 301 {  // å¿½ç•¥å–æ¶ˆé”™è¯¯
                    print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                }
            }
        }
    }
    
    // æš‚åœå½•éŸ³
    func pauseRecording() {
        guard isRecording && !isPaused else { return }
        
        print("â¸ï¸ æš‚åœå½•éŸ³...")
        isPaused = true
        
        // æš‚åœå½•éŸ³å™¨
        audioRecorder?.pause()
        recordingTimer?.invalidate()
        
        // æš‚åœéŸ³é¢‘å¼•æ“
        audioEngine.pause()
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
        print("âœ… å½•éŸ³å·²æš‚åœ")
    }
    
    // ç»§ç»­å½•éŸ³
    func resumeRecording() {
        guard isRecording && isPaused else { return }
        
        print("â–¶ï¸ ç»§ç»­å½•éŸ³...")
        isPaused = false
        
        // ç»§ç»­å½•éŸ³å™¨
        audioRecorder?.record()
        
        // é‡æ–°å¯åŠ¨è®¡æ—¶å™¨ - ä½¿ç”¨ common æ¨¡å¼ç¡®ä¿åå°ç»§ç»­è¿è¡Œ
        recordingTimer = Timer(timeInterval: 0.5, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            self.recordingDuration += 0.5
            self.updateLiveActivity()
        }
        RunLoop.current.add(recordingTimer!, forMode: .common)
        
        // ç»§ç»­éŸ³é¢‘å¼•æ“
        do {
            try audioEngine.start()
        } catch {
            print("âŒ ç»§ç»­éŸ³é¢‘å¼•æ“å¤±è´¥: \(error)")
        }
        
        // æ›´æ–° Live Activity
        updateLiveActivity()
        
        print("âœ… å½•éŸ³å·²ç»§ç»­")
    }
    
    // åœæ­¢å½•éŸ³
    func stopRecording(modelContext: ModelContext? = nil) {
        print("ğŸ›‘ åœæ­¢å½•éŸ³...")
        
        guard isRecording else { return }
        
        isRecording = false
        isPaused = false
        
        // åœæ­¢å½•éŸ³å™¨
        audioRecorder?.stop()
        recordingTimer?.invalidate()
        
        // åœæ­¢éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest = nil
        
        // ä¿å­˜åˆ°ä¼šè®®çºªè¦ï¼ˆå°è¯•ä»å‚æ•°æˆ–å›è°ƒè·å– ModelContextï¼‰
        let context = modelContext ?? modelContextProvider?()
        saveToMeeting(modelContext: context)
        
        // ç»“æŸ Live Activity
        endLiveActivity()
        
        print("âœ… å½•éŸ³å·²åœæ­¢")
    }
    
    // ä¿å­˜åˆ°ä¼šè®®çºªè¦
    private func saveToMeeting(modelContext: ModelContext?) {
        guard let audioURL = audioURL,
              let modelContext = modelContext else {
            print("âŒ æ— æ³•ä¿å­˜ä¼šè®®çºªè¦")
            return
        }
        
        let meeting = Meeting(
            title: "ä¼šè®®å½•éŸ³ - \(formatDate(Date()))",
            content: recognizedText,
            audioFilePath: audioURL.path,
            createdAt: Date(),
            duration: recordingDuration
        )
        
        modelContext.insert(meeting)
        
        do {
            try modelContext.save()
            print("âœ… ä¼šè®®çºªè¦å·²ä¿å­˜")
        } catch {
            print("âŒ ä¿å­˜å¤±è´¥: \(error)")
        }
    }
    
    // MARK: - Live Activity ç®¡ç†
    
    private func startLiveActivity() {
        guard ActivityAuthorizationInfo().areActivitiesEnabled else {
            print("âš ï¸ Live Activity æœªå¯ç”¨")
            return
        }
        
        let attributes = MeetingRecordingAttributes(meetingTitle: "ä¼šè®®å½•éŸ³")
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: "å¼€å§‹å½•éŸ³...",
            duration: 0,
            isRecording: true,
            isPaused: false
        )
        
        do {
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // æœ€é«˜ä¼˜å…ˆçº§ï¼Œä¿æŒå±•å¼€çŠ¶æ€
            )
            
            activity = try Activity<MeetingRecordingAttributes>.request(
                attributes: attributes,
                content: activityContent,
                pushType: nil
            )
            print("âœ… Live Activity å·²å¯åŠ¨ï¼ˆå±•å¼€æ¨¡å¼ï¼‰")
        } catch {
            print("âŒ Live Activity å¯åŠ¨å¤±è´¥: \(error)")
        }
    }
    
    private func updateLiveActivity() {
        guard let activity = activity else { return }
        
        let contentState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText.isEmpty ? "ç­‰å¾…è¯´è¯..." : recognizedText,
            duration: recordingDuration,
            isRecording: isRecording,
            isPaused: isPaused
        )
        
        Task { @MainActor in
            // åˆ›å»º ActivityContentï¼Œè®¾ç½®é«˜ä¼˜å…ˆçº§ä¿æŒå±•å¼€çŠ¶æ€
            let activityContent = ActivityContent(
                state: contentState,
                staleDate: nil,
                relevanceScore: 100.0  // ä¿æŒæœ€é«˜ä¼˜å…ˆçº§
            )
            await activity.update(activityContent)
        }
    }
    
    private func endLiveActivity() {
        guard let activity = activity else { return }
        
        let finalState = MeetingRecordingAttributes.ContentState(
            transcribedText: recognizedText,
            duration: recordingDuration,
            isRecording: false,
            isPaused: false
        )
        
        Task {
            await activity.end(using: finalState, dismissalPolicy: .after(.now + 3))
            print("âœ… Live Activity å·²ç»“æŸ")
        }
        
        self.activity = nil
    }
    
    // MARK: - åå°å¤„ç†
    
    private func setupBackgroundHandling() {
        // ç›‘å¬appè¿›å…¥åå°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppDidEnterBackground),
            name: UIApplication.didEnterBackgroundNotification,
            object: nil
        )
        
        // ç›‘å¬appè¿›å…¥å‰å°
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillEnterForeground),
            name: UIApplication.willEnterForegroundNotification,
            object: nil
        )
        
        // ç›‘å¬appå³å°†ç»ˆæ­¢
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAppWillTerminate),
            name: UIApplication.willTerminateNotification,
            object: nil
        )
        
        // ç›‘å¬éŸ³é¢‘ä¸­æ–­
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioInterruption),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
    }
    
    @objc private func handleAppDidEnterBackground() {
        print("ğŸ“± Appè¿›å…¥åå°ï¼Œç¡®ä¿å½•éŸ³ç»§ç»­...")
        
        guard isRecording else { return }
        
        // ç¡®ä¿éŸ³é¢‘ä¼šè¯ä¿æŒæ´»è·ƒ
        do {
            try AVAudioSession.sharedInstance().setActive(true)
        } catch {
            print("âŒ åå°éŸ³é¢‘ä¼šè¯æ¿€æ´»å¤±è´¥: \(error)")
        }
        
        // ç«‹å³æ›´æ–°Live Activity
        updateLiveActivity()
    }
    
    @objc private func handleAppWillEnterForeground() {
        print("ğŸ“± Appå›åˆ°å‰å°")
        
        // æ›´æ–°Live ActivityçŠ¶æ€
        if isRecording {
            updateLiveActivity()
        }
    }
    
    @objc private func handleAppWillTerminate() {
        print("ğŸš¨ Appå³å°†ç»ˆæ­¢ï¼Œè‡ªåŠ¨ä¿å­˜å½•éŸ³")
        
        // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œç«‹å³åœæ­¢å¹¶ä¿å­˜
        if isRecording {
            // è·å– ModelContext
            let context = modelContextProvider?()
            
            // åŒæ­¥åœæ­¢å½•éŸ³ï¼ˆå› ä¸ºæ—¶é—´ç´§è¿«ï¼‰
            isRecording = false
            isPaused = false
            
            // åœæ­¢å½•éŸ³å™¨
            audioRecorder?.stop()
            recordingTimer?.invalidate()
            
            // åœæ­¢éŸ³é¢‘å¼•æ“
            if audioEngine.isRunning {
                audioEngine.stop()
                audioEngine.inputNode.removeTap(onBus: 0)
            }
            
            recognitionRequest?.endAudio()
            recognitionTask?.cancel()
            recognitionTask = nil
            recognitionRequest = nil
            
            // ä¿å­˜åˆ°æ•°æ®åº“
            saveToMeeting(modelContext: context)
            
            // ç»“æŸ Live Activity
            if let activity = activity {
                let finalState = MeetingRecordingAttributes.ContentState(
                    transcribedText: recognizedText,
                    duration: recordingDuration,
                    isRecording: false,
                    isPaused: false
                )
                
                Task {
                    await activity.end(using: finalState, dismissalPolicy: .immediate)
                }
            }
            
            print("âœ… å½•éŸ³å·²è‡ªåŠ¨ä¿å­˜ï¼ˆAppç»ˆæ­¢ï¼‰")
        }
    }
    
    @objc private func handleAudioInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            print("âš ï¸ éŸ³é¢‘ä¸­æ–­å¼€å§‹")
            if isRecording && !isPaused {
                pauseRecording()
            }
            
        case .ended:
            print("âœ… éŸ³é¢‘ä¸­æ–­ç»“æŸ")
            if let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                if options.contains(.shouldResume) && isPaused {
                    resumeRecording()
                }
            }
            
        @unknown default:
            break
        }
    }
    
    // MARK: - è¾…åŠ©æ–¹æ³•
    
    private func formatDate(_ date: Date) -> String {
        let formatter = DateFormatter()
        formatter.dateFormat = "MMæœˆddæ—¥ HH:mm"
        return formatter.string(from: date)
    }
    
    private func ensureRecordingsFolder() -> URL {
        let documentsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
        let folderURL = documentsURL.appendingPathComponent("MeetingRecordings", isDirectory: true)
        
        if !FileManager.default.fileExists(atPath: folderURL.path) {
            do {
                try FileManager.default.createDirectory(at: folderURL, withIntermediateDirectories: true)
            } catch {
                print("âŒ åˆ›å»ºå½•éŸ³ç›®å½•å¤±è´¥: \(error)")
            }
        }
        
        return folderURL
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
    }
}

