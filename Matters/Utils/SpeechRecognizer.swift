import Foundation
import Speech
import AVFoundation

class SpeechRecognizer: ObservableObject {
    @Published var isRecording = false
    @Published var recognizedText = ""
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private var shouldAcceptUpdates = false  // æ˜¯å¦æ¥å—è¯†åˆ«å›è°ƒçš„æ›´æ–°
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("âœ… è¯­éŸ³è¯†åˆ«æƒé™å·²æˆæƒ")
                case .denied, .restricted, .notDetermined:
                    print("âŒ è¯­éŸ³è¯†åˆ«æƒé™æœªæˆæƒ")
                @unknown default:
                    break
                }
            }
        }
    }
    
    func startRecording(onTextUpdate: @escaping (String) -> Void) {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("âŒ è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨")
            return
        }
        
        // åœæ­¢ä¹‹å‰çš„ä»»åŠ¡
        stopRecording()
        
        // é…ç½®éŸ³é¢‘ä¼šè¯
        let audioSession = AVAudioSession.sharedInstance()
        do {
            // ä½¿ç”¨æ ‡å‡†çš„å½•éŸ³é…ç½®ï¼Œç®€å•å¯é 
            try audioSession.setCategory(.record, mode: .default, options: .duckOthers)
            // æ¿€æ´»éŸ³é¢‘ä¼šè¯
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("âŒ éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
            return
        }
        
        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            print("âŒ æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚")
            return
        }
        
        // å¯ç”¨å®æ—¶è¯†åˆ«ç»“æœ
        recognitionRequest.shouldReportPartialResults = true
        // æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥æé«˜è¯†åˆ«å‡†ç¡®åº¦
        if #available(iOS 16.0, *) {
            recognitionRequest.addsPunctuation = true  // è‡ªåŠ¨æ·»åŠ æ ‡ç‚¹ç¬¦å·
        }
        // ä½¿ç”¨è®¾å¤‡ç«¯è¯†åˆ«ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œæé«˜éšç§æ€§å’Œé€Ÿåº¦
        if #available(iOS 13.0, *) {
            recognitionRequest.requiresOnDeviceRecognition = false  // å…ˆå°è¯•äº‘ç«¯ï¼Œè·å¾—æ›´å¥½çš„å‡†ç¡®åº¦
        }
        
        // é…ç½®éŸ³é¢‘å¼•æ“
        let inputNode = audioEngine.inputNode
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        
        // ä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒºï¼ˆ4096ï¼‰ä»¥è·å¾—æ›´å¥½çš„éŸ³é¢‘è´¨é‡å’Œè¿ç»­æ€§
        inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { buffer, _ in
            recognitionRequest.append(buffer)
        }
        
        audioEngine.prepare()
        
        do {
            try audioEngine.start()
            isRecording = true
            shouldAcceptUpdates = true  // å¼€å§‹æ¥å—æ›´æ–°
            print("ğŸ¤ å¼€å§‹å½•éŸ³")
        } catch {
            print("âŒ å¯åŠ¨éŸ³é¢‘å¼•æ“å¤±è´¥: \(error)")
            return
        }
        
        // å¼€å§‹è¯†åˆ«
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }
            
            // åªåœ¨å…è®¸æ›´æ–°æ—¶å¤„ç†è¯†åˆ«ç»“æœ
            if let result = result, self.shouldAcceptUpdates {
                let text = result.bestTranscription.formattedString
                DispatchQueue.main.async {
                    self.recognizedText = text
                    onTextUpdate(text)
                }
            }
            
            if let error = error {
                // Code 301 æ˜¯æ‰‹åŠ¨åœæ­¢å½•éŸ³çš„æ­£å¸¸é”™è¯¯ï¼Œä¸éœ€è¦æ‰“å°
                let nsError = error as NSError
                if nsError.code == 301 || nsError.domain == "kLSRErrorDomain" && error.localizedDescription.contains("canceled") {
                    // æ­£å¸¸çš„åœæ­¢å½•éŸ³æ“ä½œï¼Œå¿½ç•¥
                    return
                }
                
                // å…¶ä»–é”™è¯¯æ‰æ‰“å°å¹¶åœæ­¢
                print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                self.stopRecording()
            }
        }
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        isRecording = false
        shouldAcceptUpdates = false  // ç«‹å³åœæ­¢æ¥å—æ›´æ–°ï¼Œé˜²æ­¢åç»­å›è°ƒè¦†ç›–å·²è¯†åˆ«çš„æ–‡å­—
        
        print("ğŸ›‘ åœæ­¢å½•éŸ³")
        
        // å…ˆåœæ­¢éŸ³é¢‘å¼•æ“å’Œç§»é™¤tap
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // ç»“æŸéŸ³é¢‘è¾“å…¥
        recognitionRequest?.endAudio()
        
        // å»¶è¿Ÿæ¸…ç†è¯†åˆ«ä»»åŠ¡
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) { [weak self] in
            self?.recognitionRequest = nil
            self?.recognitionTask?.finish()
            self?.recognitionTask = nil
        }
        
        // é‡ç½®éŸ³é¢‘ä¼šè¯
        try? AVAudioSession.sharedInstance().setActive(false)
    }
    
    // è¯†åˆ«å½•éŸ³æ–‡ä»¶ï¼ˆä½¿ç”¨è‹¹æœåŸå§‹æ¡†æ¶ï¼Œæ•´æ®µï¼‰
    static func transcribeAudioFile(audioURL: URL) async throws -> String {
        // è¯·æ±‚æƒé™
        let authStatus = await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
        
        guard authStatus == .authorized else {
            throw NSError(domain: "SpeechRecognizer", code: -1, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«æƒé™æœªæˆæƒ"])
        }
        
        // åˆ›å»ºè¯†åˆ«å™¨
        guard let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN")),
              recognizer.isAvailable else {
            throw NSError(domain: "SpeechRecognizer", code: -2, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨"])
        }
        
        // åˆ›å»ºæ–‡ä»¶è¯†åˆ«è¯·æ±‚
        let request = SFSpeechURLRecognitionRequest(url: audioURL)
        request.shouldReportPartialResults = false  // æ–‡ä»¶è¯†åˆ«ä¸éœ€è¦éƒ¨åˆ†ç»“æœ
        
        // å¯ç”¨æ ‡ç‚¹ç¬¦å·ï¼ˆiOS 16+ï¼‰
        if #available(iOS 16.0, *) {
            request.addsPunctuation = true
        }
        
        // æ‰§è¡Œè¯†åˆ«
        return try await withCheckedThrowingContinuation { continuation in
            var hasResumed = false
            
            recognizer.recognitionTask(with: request) { result, error in
                if let error = error {
                    let nsError = error as NSError
                    // å¿½ç•¥å–æ¶ˆé”™è¯¯ï¼ˆcode 301ï¼‰
                    if nsError.code == 301 {
                        return
                    }
                    if !hasResumed {
                        hasResumed = true
                        continuation.resume(throwing: error)
                    }
                    return
                }
                
                if let result = result {
                    let text = result.bestTranscription.formattedString
                    // æ–‡ä»¶è¯†åˆ«æ—¶ï¼ŒshouldReportPartialResults=falseï¼Œæ‰€ä»¥é€šå¸¸åªæœ‰ä¸€æ¬¡å›è°ƒä¸”isFinal=true
                    if result.isFinal || !text.isEmpty {
                        if !hasResumed {
                            hasResumed = true
                            continuation.resume(returning: text)
                        }
                    }
                }
            }
        }
    }
    
    /// åˆ†æ®µè¯†åˆ«å½•éŸ³æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§è¯†åˆ«è¿‡é•¿éŸ³é¢‘å¯¼è‡´è‹¹æœæœåŠ¡æŠ¥é”™
    /// - Parameters:
    ///   - audioURL: åŸå§‹å½•éŸ³æ–‡ä»¶
    ///   - segmentDuration: æ¯æ®µæœ€é•¿æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
    static func transcribeAudioFileInSegments(
        audioURL: URL,
        segmentDuration: TimeInterval = 5 * 60
    ) async throws -> String {
        let asset = AVURLAsset(url: audioURL)
        let totalSeconds = CMTimeGetSeconds(asset.duration)
        
        // å¦‚æœæ€»æ—¶é•¿æœ¬èº«ä¸é•¿ï¼Œå°±æŒ‰æ•´æ®µè¯†åˆ«å³å¯
        if totalSeconds.isNaN || totalSeconds <= segmentDuration {
            return try await transcribeAudioFile(audioURL: audioURL)
        }
        
        let timescale = asset.duration.timescale == 0 ? CMTimeScale(NSEC_PER_SEC) : asset.duration.timescale
        let segmentCount = Int(ceil(totalSeconds / segmentDuration))
        var allText: [String] = []
        let tempDir = FileManager.default.temporaryDirectory
        let baseName = audioURL.deletingPathExtension().lastPathComponent
        
        for index in 0..<segmentCount {
            let start = Double(index) * segmentDuration
            if start >= totalSeconds { break }
            
            let remaining = totalSeconds - start
            let currentDuration = min(segmentDuration, remaining)
            
            let startTime = CMTime(seconds: start, preferredTimescale: timescale)
            let durationTime = CMTime(seconds: currentDuration, preferredTimescale: timescale)
            let timeRange = CMTimeRange(start: startTime, duration: durationTime)
            
            guard let exporter = AVAssetExportSession(asset: asset, presetName: AVAssetExportPresetAppleM4A) else {
                continue
            }
            
            let outputURL = tempDir.appendingPathComponent("\(baseName)_part_\(index).m4a")
            // æ¸…ç†æ—§æ–‡ä»¶
            try? FileManager.default.removeItem(at: outputURL)
            
            exporter.outputURL = outputURL
            exporter.outputFileType = .m4a
            exporter.timeRange = timeRange
            
            try await withCheckedThrowingContinuation { (continuation: CheckedContinuation<Void, Error>) in
                exporter.exportAsynchronously {
                    switch exporter.status {
                    case .completed:
                        continuation.resume()
                    case .failed, .cancelled:
                        let error = exporter.error ?? NSError(domain: "SpeechRecognizer", code: -3, userInfo: [NSLocalizedDescriptionKey: "éŸ³é¢‘åˆ†æ®µå¯¼å‡ºå¤±è´¥"])
                        continuation.resume(throwing: error)
                    default:
                        // å…¶ä»–çŠ¶æ€ç†è®ºä¸Šä¸ä¼šåœ¨å›è°ƒé‡Œå‡ºç°ï¼Œè¿™é‡Œå…œåº•
                        let error = NSError(domain: "SpeechRecognizer", code: -4, userInfo: [NSLocalizedDescriptionKey: "æœªçŸ¥å¯¼å‡ºçŠ¶æ€"])
                        continuation.resume(throwing: error)
                    }
                }
            }
            
            // å¯¹å½“å‰ç‰‡æ®µåšè¯†åˆ«
            let text = try await transcribeAudioFile(audioURL: outputURL)
            if !text.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
                allText.append(text)
            }
            
            // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try? FileManager.default.removeItem(at: outputURL)
        }
        
        let merged = allText.joined(separator: "\n")
        if merged.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
            throw NSError(domain: "SpeechRecognizer", code: -5, userInfo: [NSLocalizedDescriptionKey: "åˆ†æ®µè¯†åˆ«ç»“æœä¸ºç©º"])
        }
        return merged
    }
}

