import Foundation
import Speech
import AVFoundation

// ä¸º AVAssetExportSession æä¾›ä¸€ä¸ªç®€å•çš„åŒ…è£…ç±»å‹ï¼Œæ ‡è®°ä¸º @unchecked Sendableï¼Œ
// é¿å…ç›´æ¥ä¸ºç³»ç»Ÿç±»å‹æ‰©å±• Sendable å¸¦æ¥çš„è­¦å‘Šã€‚
private final class ExportSessionBox: @unchecked Sendable {
    let exporter: AVAssetExportSession
    
    init(_ exporter: AVAssetExportSession) {
        self.exporter = exporter
    }
}

class SpeechRecognizer: ObservableObject {
    @Published var isRecording = false
    @Published var recognizedText = ""
    @Published var audioLevel: Float = 0.0
    
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    // ç‹¬ç«‹éŸ³é¢‘é˜Ÿåˆ—ï¼Œé¿å…ä¸»çº¿ç¨‹è¢«éŸ³é¢‘ä¼šè¯/å¼•æ“é˜»å¡
    private let audioQueue = DispatchQueue(label: "com.yuanyuan.speech.audio")
    // ä¼šè¯é…ç½®/æ¿€æ´»çŠ¶æ€ï¼Œé¿å…æ¯æ¬¡é‡å¤é…ç½®å¯¼è‡´å¡é¡¿
    private var isSessionConfigured = false
    private var isSessionActive = false
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    private var shouldAcceptUpdates = false  // æ˜¯å¦æ¥å—è¯†åˆ«å›è°ƒçš„æ›´æ–°
    
    // å¹³æ»‘å¤„ç†å‚æ•°
    private var smoothedLevel: Float = 0
    private let smoothingFactor: Float = 0.3  // 0~1, è¶Šå°è¶Šå¹³æ»‘ï¼Œè¶Šå¤§è¶Šæ•æ„Ÿ
    
    func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("âœ… è¯­éŸ³è¯†åˆ«æƒé™å·²æˆæƒ")
                case .denied, .restricted, .notDetermined:
                    print("âŒ è¯­éŸ³è¯†åˆ«æƒé™æœªæˆæƒ")
                @unknown default:
                    break
                }
            }
        }
    }
    
    func startRecording(onTextUpdate: @escaping (String) -> Void) {
        guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
            print("âŒ è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨")
            return
        }
        
        // åœæ­¢ä¹‹å‰çš„ä»»åŠ¡
        stopRecording()
        
        // æå‰åœ¨ä¸»çº¿ç¨‹æ›´æ–°çŠ¶æ€ï¼Œè®© UI ç«‹å³åé¦ˆ
        DispatchQueue.main.async {
            self.isRecording = true
            self.shouldAcceptUpdates = true
        }
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            let audioSession = AVAudioSession.sharedInstance()
            do {
                if !self.isSessionConfigured {
                    try audioSession.setCategory(.record, mode: .default, options: .duckOthers)
                    self.isSessionConfigured = true
                }
                if !self.isSessionActive {
                    try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
                    self.isSessionActive = true
                }
            } catch {
                print("âŒ éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
                DispatchQueue.main.async {
                    self.isRecording = false
                    self.shouldAcceptUpdates = false
                }
                return
            }
            
            // åˆ›å»ºè¯†åˆ«è¯·æ±‚
            self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = self.recognitionRequest else {
                print("âŒ æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚")
                DispatchQueue.main.async {
                    self.isRecording = false
                    self.shouldAcceptUpdates = false
                }
                return
            }
            
            recognitionRequest.shouldReportPartialResults = true
            if #available(iOS 16.0, *) {
                recognitionRequest.addsPunctuation = true
            }
            if #available(iOS 13.0, *) {
                recognitionRequest.requiresOnDeviceRecognition = false
            }
            
            let inputNode = self.audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
                recognitionRequest.append(buffer)
                
                guard let self = self else { return }
                let level = self.calculateAudioLevel(buffer: buffer)
                DispatchQueue.main.async {
                    self.audioLevel = level
                }
            }
            
            self.audioEngine.prepare()
            
            do {
                try self.audioEngine.start()
                print("ğŸ¤ å¼€å§‹å½•éŸ³")
            } catch {
                print("âŒ å¯åŠ¨éŸ³é¢‘å¼•æ“å¤±è´¥: \(error)")
                DispatchQueue.main.async {
                    self.isRecording = false
                    self.shouldAcceptUpdates = false
                }
                return
            }
            
            self.recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                guard let self = self else { return }
                
                if let result = result, self.shouldAcceptUpdates {
                    let text = result.bestTranscription.formattedString
                    DispatchQueue.main.async {
                        self.recognizedText = text
                        onTextUpdate(text)
                    }
                }
                
                if let error = error {
                    let nsError = error as NSError
                    if nsError.code == 301 || nsError.domain == "kLSRErrorDomain" && error.localizedDescription.contains("canceled") {
                        return
                    }
                    
                    print("âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯: \(error)")
                    self.stopRecording()
                }
            }
        }
    }
    
    private func calculateAudioLevel(buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return smoothedLevel }
        let frames = Int(buffer.frameLength)
        guard frames > 0 else { return smoothedLevel }
        
        // è®¡ç®—å³°å€¼å’ŒRMSçš„æ··åˆå€¼
        var sum: Float = 0
        var peak: Float = 0
        
        // å…¨é‡é‡‡æ ·ä»¥è·å¾—æœ€ç²¾ç¡®çš„ç»“æœ
        for i in 0..<frames {
            let sample = abs(channelData[i])
            sum += sample * sample
            if sample > peak {
                peak = sample
            }
        }
        
        let rms = sqrt(sum / Float(frames))
        
        // æ··åˆRMSå’Œå³°å€¼
        let rawLevel = rms * 0.6 + peak * 0.4
        
        // æé«˜å™ªå£°é—¨é™ï¼Œè¿‡æ»¤ç¯å¢ƒå™ªéŸ³ï¼ˆè¯´è¯æ—¶ä¸€èˆ¬ > 0.03ï¼‰
        let gatedLevel = rawLevel < 0.025 ? 0 : rawLevel
        
        // çº¿æ€§æ”¾å¤§ï¼Œä¸è¦å¤ªæ¿€è¿›
        let amplifiedLevel = gatedLevel * 3.0
        
        // é™åˆ¶åœ¨0~1èŒƒå›´
        let clampedLevel = min(amplifiedLevel, 1.0)
        
        // å¹³æ»‘å¤„ç†ï¼šä¸Šå‡å¿«ï¼Œä¸‹é™å¿«ï¼ˆè®©é™éŸ³æ—¶å¿«é€Ÿå½’é›¶ï¼‰
        if clampedLevel > smoothedLevel {
            smoothedLevel = smoothedLevel + (clampedLevel - smoothedLevel) * 0.5
        } else {
            // ä¸‹é™æ›´å¿«ï¼Œè®©é™éŸ³æ£€æµ‹æ›´çµæ•
            smoothedLevel = smoothedLevel + (clampedLevel - smoothedLevel) * 0.4
        }
        
        return smoothedLevel
    }
    
    func stopRecording() {
        guard isRecording else { return }
        
        DispatchQueue.main.async {
            self.isRecording = false
            self.audioLevel = 0
            self.smoothedLevel = 0
            self.shouldAcceptUpdates = false
        }
        
        print("ğŸ›‘ åœæ­¢å½•éŸ³")
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            if self.audioEngine.isRunning {
                self.audioEngine.stop()
                self.audioEngine.inputNode.removeTap(onBus: 0)
            }
            
            self.recognitionRequest?.endAudio()
            
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.3) { [weak self] in
                self?.recognitionRequest = nil
                self?.recognitionTask?.finish()
                self?.recognitionTask = nil
            }
            
            // ä¿æŒä¼šè¯æ´»è·ƒï¼Œé¿å…ä¸‹æ¬¡é‡æ–°æ¿€æ´»å¯¼è‡´å»¶è¿Ÿ
            // ä»…åœ¨ app é€€å‡ºå½•éŸ³åœºæ™¯æ—¶ï¼ˆå¦‚åå°/é€€å‡ºï¼‰å†ç»Ÿä¸€æ”¶å›
        }
    }
    
    // è¯†åˆ«å½•éŸ³æ–‡ä»¶ï¼ˆä½¿ç”¨è‹¹æœåŸå§‹æ¡†æ¶ï¼Œæ•´æ®µï¼‰
    static func transcribeAudioFile(audioURL: URL) async throws -> String {
        // è¯·æ±‚æƒé™
        let authStatus = await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
        
        guard authStatus == .authorized else {
            throw NSError(domain: "SpeechRecognizer", code: -1, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«æƒé™æœªæˆæƒ"])
        }
        
        // åˆ›å»ºè¯†åˆ«å™¨
        guard let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN")),
              recognizer.isAvailable else {
            throw NSError(domain: "SpeechRecognizer", code: -2, userInfo: [NSLocalizedDescriptionKey: "è¯­éŸ³è¯†åˆ«å™¨ä¸å¯ç”¨"])
        }
        
        // åˆ›å»ºæ–‡ä»¶è¯†åˆ«è¯·æ±‚
        let request = SFSpeechURLRecognitionRequest(url: audioURL)
        request.shouldReportPartialResults = false  // æ–‡ä»¶è¯†åˆ«ä¸éœ€è¦éƒ¨åˆ†ç»“æœ
        
        // å¯ç”¨æ ‡ç‚¹ç¬¦å·ï¼ˆiOS 16+ï¼‰
        if #available(iOS 16.0, *) {
            request.addsPunctuation = true
        }
        
        // æ‰§è¡Œè¯†åˆ«
        return try await withCheckedThrowingContinuation { continuation in
            var hasResumed = false
            
            recognizer.recognitionTask(with: request) { result, error in
                if let error = error {
                    let nsError = error as NSError
                    // å¿½ç•¥å–æ¶ˆé”™è¯¯ï¼ˆcode 301ï¼‰
                    if nsError.code == 301 {
                        return
                    }
                    if !hasResumed {
                        hasResumed = true
                        continuation.resume(throwing: error)
                    }
                    return
                }
                
                if let result = result {
                    let text = result.bestTranscription.formattedString
                    // æ–‡ä»¶è¯†åˆ«æ—¶ï¼ŒshouldReportPartialResults=falseï¼Œæ‰€ä»¥é€šå¸¸åªæœ‰ä¸€æ¬¡å›è°ƒä¸”isFinal=true
                    if result.isFinal || !text.isEmpty {
                        if !hasResumed {
                            hasResumed = true
                            continuation.resume(returning: text)
                        }
                    }
                }
            }
        }
    }
    
    /// åˆ†æ®µè¯†åˆ«å½•éŸ³æ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§è¯†åˆ«è¿‡é•¿éŸ³é¢‘å¯¼è‡´è‹¹æœæœåŠ¡æŠ¥é”™
    /// - Parameters:
    ///   - audioURL: åŸå§‹å½•éŸ³æ–‡ä»¶
    ///   - segmentDuration: æ¯æ®µæœ€é•¿æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 5 åˆ†é’Ÿ
    static func transcribeAudioFileInSegments(
        audioURL: URL,
        segmentDuration: TimeInterval = 5 * 60
    ) async throws -> String {
        let asset = AVURLAsset(url: audioURL)
        
        // ä½¿ç”¨æ–°çš„å¼‚æ­¥å±æ€§åŠ è½½ durationï¼Œå…¼å®¹æ—§ç³»ç»Ÿ
        let durationTime: CMTime
        if #available(iOS 16.0, *) {
            durationTime = try await asset.load(.duration)
        } else {
            durationTime = asset.duration
        }
        
        let totalSeconds = CMTimeGetSeconds(durationTime)
        
        // å¦‚æœæ€»æ—¶é•¿æœ¬èº«ä¸é•¿ï¼Œå°±æŒ‰æ•´æ®µè¯†åˆ«å³å¯
        if totalSeconds.isNaN || totalSeconds <= segmentDuration {
            return try await transcribeAudioFile(audioURL: audioURL)
        }
        
        let timescale = durationTime.timescale == 0 ? CMTimeScale(NSEC_PER_SEC) : durationTime.timescale
        let segmentCount = Int(ceil(totalSeconds / segmentDuration))
        var allText: [String] = []
        let tempDir = FileManager.default.temporaryDirectory
        let baseName = audioURL.deletingPathExtension().lastPathComponent
        
        for index in 0..<segmentCount {
            let start = Double(index) * segmentDuration
            if start >= totalSeconds { break }
            
            let remaining = totalSeconds - start
            let currentDuration = min(segmentDuration, remaining)
            
            let startTime = CMTime(seconds: start, preferredTimescale: timescale)
            let durationTime = CMTime(seconds: currentDuration, preferredTimescale: timescale)
            let timeRange = CMTimeRange(start: startTime, duration: durationTime)
            
            let outputURL = tempDir.appendingPathComponent("\(baseName)_part_\(index).m4a")
            // æ¸…ç†æ—§æ–‡ä»¶
            try? FileManager.default.removeItem(at: outputURL)
            
            guard let exporter = AVAssetExportSession(asset: asset, presetName: AVAssetExportPresetAppleM4A) else {
                continue
            }
            
            exporter.timeRange = timeRange
            
            if #available(iOS 18.0, *) {
                // iOS 18 åŠä»¥ä¸Šä½¿ç”¨æ–°çš„å¼‚æ­¥å¯¼å‡º APIï¼Œé¿å…åºŸå¼ƒè­¦å‘Š
                try await exporter.export(to: outputURL, as: .m4a)
            } else {
                exporter.outputURL = outputURL
                exporter.outputFileType = .m4a
                
                let exporterBox = ExportSessionBox(exporter)
                
                try await withCheckedThrowingContinuation { (continuation: CheckedContinuation<Void, Error>) in
                    exporterBox.exporter.exportAsynchronously {
                        let exporter = exporterBox.exporter
                        switch exporter.status {
                        case .completed:
                            continuation.resume()
                        case .failed, .cancelled:
                            let error = exporter.error ?? NSError(domain: "SpeechRecognizer", code: -3, userInfo: [NSLocalizedDescriptionKey: "éŸ³é¢‘åˆ†æ®µå¯¼å‡ºå¤±è´¥"])
                            continuation.resume(throwing: error)
                        default:
                            // å…¶ä»–çŠ¶æ€ç†è®ºä¸Šä¸ä¼šåœ¨å›è°ƒé‡Œå‡ºç°ï¼Œè¿™é‡Œå…œåº•
                            let error = NSError(domain: "SpeechRecognizer", code: -4, userInfo: [NSLocalizedDescriptionKey: "æœªçŸ¥å¯¼å‡ºçŠ¶æ€"])
                            continuation.resume(throwing: error)
                        }
                    }
                }
            }
            
            // å¯¹å½“å‰ç‰‡æ®µåšè¯†åˆ«
            let text = try await transcribeAudioFile(audioURL: outputURL)
            if !text.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
                allText.append(text)
            }
            
            // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try? FileManager.default.removeItem(at: outputURL)
        }
        
        let merged = allText.joined(separator: "\n")
        if merged.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
            throw NSError(domain: "SpeechRecognizer", code: -5, userInfo: [NSLocalizedDescriptionKey: "åˆ†æ®µè¯†åˆ«ç»“æœä¸ºç©º"])
        }
        return merged
    }
}

